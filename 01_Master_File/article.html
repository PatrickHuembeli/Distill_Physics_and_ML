<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. –
-->
<!doctype html>

<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <script src='https://d3js.org/d3.v4.min.js' charset="utf-8"></script>
<!--   <script src="https://d3js.org/d3-selection-multi.v0.4.min.js"></script> -->

<script>
  function MathCache(id) {
    return document.querySelector("#math-cache ." + id).innerHTML;
  }
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1" >
  <meta charset="utf8">
  <link href="style.css" rel="stylesheet" type="text/css">
</head>
<body>
  <distill-header></distill-header>
<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
    "title": "Physics and Machine Learning: Insights from Energy-based Models",
    "description": "We need to understand the simple models",
    "published": "XXX xx, 2019",
    "authors": [
      {
        "author":"Patrick Huembeli",
        "authorURL":"http://patrickhuembeli.github.io/",
        "affiliations": [{"name": "ICFO-The Institute for Photonics", "url": "https://icfo.eu/"}]
      },
      {
        "author":"Juan Miguel Arrazola",
        "affiliations": [{"name": "Xanadu", "url": "https://www.xanadu.ai/"}]
      },
      {
        "author":"Nathan Killoran",
        "authorURL":"https://github.com/co9olguy",
        "affiliations": [{"name": "Xanadu", "url": "https://www.xanadu.ai/"}]
      },
      {
        "author":"Peter Wittek",
        "authorURL":"https://peterwittek.com/",
        "affiliations": [
          {"name": "University of Toronto", "url": "https://www.rotman.utoronto.ca/"},
          {"name": "Creative Destruction Lab", "url": "https://www.creativedestructionlab.com/"},
          {"name": "Vector Institute for Artificial Intelligence", "url": "https://vectorinstitute.ai/"},
          {"name": "Perimeter Institute for Theoretical Physics", "url": "https://perimeterinstitute.ca/"}
        ]
      },
      {
        "author":"Masoud Mohseni",
        "affiliations": [
          {"name": "Google AI Quantum", "url": "https://ai.google/research/teams/applied-science/quantum-ai/"}
        ]
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
</d-front-matter>
<!-- Put Title, short abstract and image -->
<d-title>
<!--   <figure style="grid-column: page; margin: 1rem 0;"><img src="momentum.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure> -->







    <figure style = "grid-column: page; margin: 1rem 0;" id = "Teaser_Id">
  <div id = "Equilibration_Img_Teaser" style="left:0px; top:0px"></div>
  </figure>

  <script type = "text/javascript" src = "Equilibration_Teaser.js" ></script>
  <p>In this post we will give an introduction into energy based models and especially into Boltzmann machines from the perspective of physics. These models show interesting behaviours with which we attempt to describe basic ideas of learning but also show the limitations of energy based models. We will continuously draw the connections to machine learning and finally also show how energy based models can overcome certain problems that appear in generative models in deep learning such as GANs.</p>
</d-title>
<d-byline></d-byline>
<!-- Actual Article starts here -->
<d-article>
<!--  INTRODUCTION -->
  <a class="marker" href="#section-1" id="section-1"><span>Introduction</span></a>

  <p> <b>PH: We need to add "why are EBMs important" and we should mention that we are mostly looking at Hopfield and BM and why.</b> </p>
  <p>Inspiration takes many forms. A painter beholds a sunrise and translates its colours onto a canvas. A writer sets sail on a new adventure, transforming life experiences into compelling stories. A scientist attempting to design artificial intelligence systems; where do they look for inspiration? </p>
    <!-- A possible starting point is to contemplate intelligence as it manifests itself in nature: living organisms that are capable of creativity and learning. But intelligent beings are extraordinarily complex. Can we instead draw inspiration from simpler, more fundamental systems? -->

  <p> In this article, we take a journey to understand the interplay between physics and machine learning. Beginning from an attempt to understand how simple forms of artificial intelligence can originate from the collective behaviour of interacting particles, we uncover fundamental physical concepts underlying the theory and practice of energy-based models. This includes the design, implementation, and training of Hopfield networks and Boltzmann machines, where we explain familiar concepts from new perspectives. Along the way, we highlight how physical principles can explain both the success and challenges associated with these models. Finally, we conclude by discussing the role of physics in shaping the next generation of machine learning algorithms. <b>JM: Intro may need more work.</b> </p>


<!-- Second Section  2  -->
  <a class="marker" href="#section-2" id="section-2"><span>Energy-based models</span></a>
  <h2 id="testing-the-bm-as-a-trained-model">Energy-based models</h2>

  <p>
  Energy-based models emerged in the machine learning literature in the 1980s <d-cite key="hopfield_neural_1982"></d-cite>.
  They were further developed, extended, and improved, over several decades. Some energy-based models, Boltzmann machines especially, have recently been “back-ported” to physics, where they have successfully been used to model the wavefunctions of quantum systems <d-cite key="carleo2017solving"></d-cite>. They also seem to get back into the focus of fundamental research in machine learning. <d-footnote>According to this <a href="https://twitter.com/datasciencenig/status/1020355546581553152" target="_blank">tweet</a>, Yoshua Bengio as an example wants to return to studying Boltzmann machines for a better understanding of generative models.</d-footnote> Furthermore Boltzmann machines are still being employed in other areas, such as recommendation systems <d-cite key='salakhutdinov2007restricted'></d-cite> and they even become competitive with GANs <d-cite key="du2019implicit"></d-cite>. The rekindled interest is understandable: many types of energy-based models are generative, guarantee to sample a valid probability distribution, and they can often be stacked to create deep, hierarchical representations <d-cite key="le2010deep"></d-cite>. <b>JM: Should probably expand this paragraph or move it elsewhere. </b> </p>

  <p>
  To understand the origin of these models, imagine being an ambitious experimental scientist attempting to build a physical system that is capable of intelligent behavior. This is indeed an ambitious goal: being able to fully control a large collection of particles is challenging; typically they move randomly as they interact with the environment. Therefore, as an initial strategic choice, we aim to use these random fluctuations to our advantage and design <em>probabilistic systems</em>. Mathematically, these are characterized by a probability distribution that determines the likely configurations of the system at different times. The challenge is to design systems that are sufficiently complex to give rise to rich behaviour, but also simple enough that they can be efficiently trained and characterized. </p>

<p>
For large systems, it is overwhelmingly difficult to keep track of all its rapidly-fluctuating internal degrees of freedom; we typically can only expect to have access to coarse-grained information, like the total energy of the system. For a given system, the laws of physics allow us to determine the system's energy for any given configuration. This is encapsulated in terms of an <em> energy function </em> <d-math> E(x) </d-math> that assigns energy values to the possible states 
<d-math> x=(x_1, x_2, \ldots, x_n) </d-math> of an <d-math>n</d-math>-particle system. Energy functions are also referred to as the <em>Hamiltonian</em> of the system. A question arises: given the constraint that the average energy <d-math> \langle E(x) \rangle </d-math> of the system is fixed, what probability distribution should be assigned to its configurations? Intuitively, it makes sense to choose the distribution with maximum entropy: if we do not have any information – and therefore no constraint – about a particular degree of freedom of a system, we should remain maximally flexible in our choice of model while remaining consistent with the quantitites that are constrained. Choosing the maximum entropy model reduces the amount of potentially biased or unsubstantiated prior information built into a model. This strategy is known as <em> Jaynes' maximum entropy principle </em>, which states that in assigning a model on the basis of partial available information, the distribution with the largest possible entropy should be chosen. The resulting distribution <d-math> P(x) </d-math> is the solution to the optimization problem

 <d-math block>
  \begin{aligned}
  &\max_{P(x)}\,\,  \sum_x -P(x)\log P(x) \\[0.4em]
   \text{s.t. }& \sum_x P(x) E(x) = \langle E \rangle,
  \end{aligned}
  </d-math>

whose solution is [cite Jaynes]
 <d-math block>
  P(x) = \frac{1}{Z} \exp\left[ - E(x)/T \right],
 </d-math>
where  <d-math>T </d-math> is a free parameter and <d-math> Z=\sum_x \exp[- E(x)/T] </d-math> is a normalization constant known as the <em>partition function</em>. This probability distribution is a familiar one in statistical physics: it is the <em> Boltzmann distribution </em>, which describes the probability of finding the system in a state <d-math>x</d-math> when it is in thermal equilibrium with a bath at temperature <d-math> T </d-math>. The Boltzmann distribution establishes a concrete relationship between energy and probability: low-energy configurations are the most likely to be observed. In the context of ML, probabilistic models governed by an energy function that describes the probability of a certain configuration are known as <em>energy-based models</em> and the Boltzmann distribution is an example of how to connect the energy with a probability. <b>JM: Check that this is factually correct. PH: Now it is closer to the truth</b>. </p>

<p>
Typically, the energy function of a physical system can be expressed as a sum over contributions arising both from the internal energy of each particle and the interactions between them. In such cases, the energy function can be written as <d-math> E(x) = \sum_i \theta_i f_i(x) </d-math> for appropriate parameters <d-math>\theta_i </d-math> and functions <d-math>f_i(x)</d-math>. The resulting Boltzmann distribution for a given temperature <d-math> T </d-math> is uniquely determined by the parameters <d-math>\theta_i </d-math> or, equivalently, by the expectation values <d-math>\langle f_i(x)\rangle </d-math>, which are the <em> sufficient statistics </em> of the distribution 
  <d-footnote>Generally for the exponential family, a statistic <d-math>T(\vec{x})</d-math> is sufficient, if we can write the probability <d-math>p(\vec{x})</d-math> as
  <d-math block>p(\vec{x}) = \exp \left( \alpha(\theta)^T T(\vec{x}) + A(\theta) \right),</d-math>
  where <d-math>\alpha(\theta)</d-math> is a vector valued function and <d-math>A(\theta)</d-math> is a scalar which for a Boltzmann distribution is simply a normalization factor that we call the partition function <d-math>A(\theta) = \log(1/Z)</d-math> <d-cite key="li2013learning"></d-cite>. Therefore in general we are not restricted to model the distribution of our data with the energy function of a classical Ising model. The choice of the constraints will determine the physical model.
  </d-footnote>.
Knowledge of the expectation values <d-math>\langle f_i(x)\rangle </d-math> fixes the parameters <d-math>\theta_i </d-math> and therefore the resulting properties of the Boltzmann distribution.
</p> 

<p>
As ambitious scientists, we are now in good shape: collections of interacting particles at thermal equilibrium lead to probabilistic models that can be characterized by a relatively small number of parameters. This paves the path to designing energy functions and training their parameters to build systems that perform intelligent tasks. Before proceeding, it is important to recognize the unique role of the temperature parameter: it determines the relative probability of observing higher-energy configurations, not just the lowest energy ones. In particular, in the limit of zero temperature, only those configurations corresponding to global minima of the energy function can be observed, while in the limit of infinite temperature, all configurations are equally likely. The role of temperature in the Boltzmann distribution is illustrated in Fig. 1. Physically, temperature quantifies the average energy of the interactions between the system and environment, which in turn lead to sporadic "jumps" towards configurations of higher energy. The higher the temperature, the more common and widespread such jumps will be.
</p>

<!-- Figure Section 2.2  -->
<div class="row">
    <div class="column_2level">
	    <p class="slider-label-text" style="width:110px">Temperature <d-math> T </d-math>:</p>
	  <p id="temperature_slider" class="slider-label-number" style="width:35px"></p>
	    <input id="temp_slider_id" type="range" onchange="temp_slider(this.value)" oninput="update_number_temp(this.value)" min="0.29" max="1" step="0.01">
	    <p class="slider-label-text" style="width:110px">Coupling <d-math> J </d-math>: </p>
	    <p id="coupling_strength" class="slider-label-number" style="width:35px"></p>
	    <input id="coupling_slider_id" type="range" onchange="coupling_slider(this.value)" oninput="update_number_coupling(this.value)" min="-1" max="1" step="0.5">
    </div>
  
    <div class="column_2level_image">
       <d-figure style = "width:270px; height:100px; display:block; margin-left:20px; margin-right:auto; position:relative" id = "RBM_complete">
       <div id = "test_figure_id" style="position:absolute; left:0px; top:0px"></div>
       </d-figure>
       <script type = "text/javascript" src = "2-level-system_coupled.js" ></script>
       </div>
     </div>

     <div class="triplecolumn"> <p class="figure-text">In the simple case of two coupled systems the sign of the coupling determines which configurations are more likely, respectively lower energy. The circles describe two systems <d-math>x_1,~x_2 </d-math> and they are colored equally if they are in the same state and they are colored differently if they are not in the same state. Physically these colors could for example be interpretd as spins. The energy function reads <d-math>E(x_1, x_2) = J x_1 x_2 </d-math> 
Low temperatures make higher energy states less likely and only the low energy configurations will appear. With increasing temperature high energy configurations get more likely. The histogram shows the probabilities of the configurations to appear at a certain coupling and temperature.<p></div>



<!-- Second Section  2  -->
  <a class="marker" href="#section-2.4" id="section-2.4"><span>Architectures</span></a>
  <h3 id="architecture">Architectures</h3>
<p>
The decision remains of what physical systems and energy functions to select. A reasonable choice is to start with arguably the simplest interesting model: a collection of particles with only two degrees of freedom, whose energy function depends on both the particle's individual state as well as the interactions between pairs of particles. To build interesting models, we can anticipate the need for interactions, since otherwise each particle behaves independently. Given this choice of system and energy funciton, the configurations of <d-math>n</d-math> particles are described in terms of the vector <d-math> \sigma=(\sigma_1, \sigma_2, \ldots, \sigma_n) </d-math>, where <d-math>\sigma_i\in\{-1, 1\}</d-math> is the state of the <d-math>i</d-math>-th particle. The resulting energy function is given by
<d-math block>
E(\sigma) = \sum_i b_i \sigma_i + \sum_{ij} w_{ij} \sigma_i\sigma_j,
</d-math> 
which is known as the <em> Ising model </em>, first introduced as a mathematical description of interacting spins in the presence of a magnetic field <d-cite key="cipra1987introduction"> </d-cite>. <b>JM: Other conventions for the Ising Hamiltonian are an option. </b> The parameters <d-math> b_i </d-math> determine the individual energies of the spins, while the parameters <d-math> w_{ij} </d-math> introduce energy contributions due pairwise interactions: for <d-math> w_{ij}>0 </d-math>, equal configurations <d-math>(\sigma_i=\sigma_j)</d-math> have higher energy, while for <d-math> w_{ij}<0 </d-math>, opposite configurations <d-math>(\sigma_i\neq\sigma_j)</d-math> lead to higher energies
<d-footnote>There are other conventions for Ising Hamiltonian in the literature, where the signs of the energy terms change. For example
<d-math block>
E(\sigma) = - \sum_i b_i \sigma_i - \sum_{ij} w_{ij} \sigma_i\sigma_j.
</d-math> 
We will follow the convention we introduced above.
</d-footnote>. 
<!--At this stage, we make another strategic choice: experimentally, it is extremely challenging to engineer physical systems that represent Ising models with arbitrary parameters. Therefore, despite our imagined role as ambitious experimental physicists, instead of building these systems in a laboratory, we aim simulate them using computer models. In particular, this gives freedom to explore a wide variety of systems without worrying about experimental constraints. --></p>

<p>
What tasks can we perform with such models? Consider the zero temperature case. At equilibrium, only the lowest-energy configurations can be observed; if the system is set to a configuration with higher energy and then allowed to equilibrate back to zero temperature, it will revert to one of the "ground states" with lowest energy. If we encode data into the ground states of the system, the model will have the ability -- through equilibration -- to retrieve data instances from incomplete or corrupted inputs, which are non-equilibrium configurations. In other words, the model can serve as an <em>auto-associative memory</em>, capable of "remembering" patterns when similar ones are given as input. Such models are known as <em>Hopfield networks</em>. The parameters <d-math> w_{ij} </d-math> in the energy function can be viewed as weighted edges in a graph and therefore the model can be represented by a network -- a neural network when particles themselves are viewed as neurons.    
</p>

<p>
Hopfield networks are not entirely general. For example, their corresponding energy function considers only pairwise interactions between spins. Nevertheless, extending the scope to more complicated models comes at a significant price as they will typically be more difficult to train, analyze, and simulate. Instead, we employ a trick: new particles can be added to the network which are not used to represent data, but whose role is to increase the overall complexity of the model. They are referred to as <em>hidden</em> nodes (as in nodes in a graph) and they act as intermediaries between the remaining <em>visible</em> nodes of the network. Each collection of hidden or visible nodes is known as a <em>layer</em>. Physically, the hidden nodes enable effective higher-order interactions between visible nodes, leading to a new effective energy function for the visible nodes. <b>JM: A reference here would be great PH: https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf here they show that RBM are universal. Here they give examples what hidden units are useful for http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf</b>. The resulting networks are called <em>Boltzmann machines</em>, in allusion to the Boltzmann distribution governing their behaviour. They are generalizations of Hopfield networks in the sense that they are contained as a special case: a Boltzman machine is equivalent to a Hopfield network when the interactions <d-math>w_{ij}</d-math> between hidden and visible nodes are set to zero. Importantly, Boltzmann machines are not only more powerful than Hopfield networks, but in a sense as powerful as they can be: they are universal approximators, in principle able to replicate any discrete probability distribution with arbitrary precision 
<d-cite key="le2008representational"></d-cite>.
</p>

<p>
Finally, as we discuss in upcoming sections, simulating and training Boltzmann machines can be challenging. To make things easier, we study models where some connections are set to zero. In the most extreme case, all intralayer connections are removed, leaving present only connections between visible and hidden nodes. The resulting models are known as <em> Restricted Boltzmann machines </em> (RBMs). Conventionally, the state of an RBM is written in terms of visible and hidden nodes, <d-math>\sigma=(v, h)</d-math> with its energy function given by <d-math>E(v, h)=\sum_i b_iv_i+\sum_j c_jh_j + \sum_{ij}w_{ij}v_ih_j</d-math>. Compared to the fully-connected Boltzmann machine, the RBM is less expressive, as there are fewer parameters. Nevertheless, the advantages gained in simulating and training these models greatly surpass the loss in expressivity, especially since, following the principles of statistical learning theory <d-cite key="neyshabur2017exploring"></d-cite>, less complex models tend to generalize better if their training error is comparable to a more complex model. The three fundamental energy-based models we study in this article, Hopfield networks, Boltzmann machines, and RBMs, are illustrated in Fig. 2.
</p>


  <div class="row">
  <div class="column">
    <figure style = "width:100%; height:160px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "architecture_Hopfield">
  <div id = "architecture_Hopfield_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  </div>
  <div class="column">
    <figure style = "width:100%; height:160px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "architecture_BM">
  <div id = "architecture_BM_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  </div>
  <div class="column">
    <figure style = "width:100%; height:160px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "architecture_RBM">
  <div id = "architecture_RBM_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  </div>
  </div>
  <script type = "text/javascript" src = "architecture.js" ></script>


<div class="row">
	<div class="column">
		<p class="figure-text">Hopfield network: The number of nodes is equal to the size of the input data. There are no hidden nodes (dashed) contributing to the energy.</p>
	</div>
	<div class="column">
    <p class="figure-text">Boltzmann machine: The Boltzmann machine graph is fully connected. The visible nodes (blue) are clamped to the input data. The hidden nodes (yellow) are free parameters.</p>
	</div>
	<div class="column">
    <p class="figure-text">Restricted Boltzmann machine: Also with hidden and visible nodes, but we don't allow connections within the same layer. This will simplify the training.
	</div>
</div>

<!-- Second Section  3.1  -->
  <a class="marker" href="#section-3.1" id="section-3.1"><span>Sampling</span></a>
  <h3 id="sampling">Sampling</h3>
<p>
From an experimental perspective, given a collection of particles governed by a specific energy function, sampling from its Boltzmann distribution at a given temperature is straightforward: just place the system in contact with an environment at the desired temperature and register the system's state at different times. The challenge we face is to simulate this process using computer algorithms. <b>JM: Is there already something to be said here about spin-glasses and/or phase transitions?</b> </p> 

<p>
Consider first the zero-temperature case. The key physical principle underlying the Boltzmann distribution is the connection between energy and probability: the likelihood of observing a specific configuration decreases exponentially with its energy. A strategy to simulate sampling from a Boltzmann distribution is to identify low-energy configurations and, depending on temperature, ocassionally select also states with higher energy. One simple strategy is to locally change the state of each particle such that it decreases the total energy of the system. For an Ising energy function, when keeping the state of all other particles fixed, the change in energy <d-math>\Delta E</d-math> introduced by changing the <d-math>i</d-math>-th particle's state from <d-math>\sigma_i</d-math> to <d-math>-\sigma_i</d-math> is given by <d-math>\Delta E_i = 2\sigma_i(b_i+\sum_j w_{ij} \sigma_j)</d-math>. With this in mind, to search for equilibrium states of the system, we iteratively apply the update rule

<d-math block>\sigma_{i}\rightarrow \left\{{\begin{array}{ll}-\sigma_i~{\text{if }} b_i+\sum_j w_{ij} \sigma_j >0,\\\sigma_i~{\text{otherwise,}}\end{array}}\right.</d-math>

Starting from a random initialization, by repeatedly applying this update rule for each particle, the system's configuration will converge to a local minimum <d-cite key="Rojas_1996_NNS"></d-cite>. This method is in general not guaranteed to find the true ground states, i.e., global minima of the energy function. <b>JM: this is a good place to mention something about successes and challenges in sampling.</b> For finite temperature, the strategy is similar, except that we allow the possibility to ocassionally jump to higher-energy configurations. In this case, the update rule is as follows: as before, if a change of state decreases the total energy, that change is applied: <d-math>\sigma_i \rightarrow -\sigma_i</d-math>. However, if a change increases the energy, i.e., if <d-math>\Delta E \geq 0</d-math>, instead of leaving the particle's state unchanged, we now flip it with probability <d-math>p = \exp(-1/T \Delta E) </d-math>. Physically, these jumps to higher-energy states mimic the random thermal fluctuations arising from exchanging energy with an environment at finite temperature. This sampling algorithm is a specific instance of the Metropolis-Hastings algorithm [], which is particularly convenient when the distribution is know up to a normalization constant, like the partitiion function, that may be challenging to compute.</p>


    <div class="row">
	 <div class="column_2level">
             <p class="slider-label-text" style="width:110px">Temperature <d-math> T </d-math>:</p>
           <p id="temperature_slider_energy_minima" class="slider-label-number" style="width:35px"></p>
             <input id="temp_slider_energy_minima_id" type="range" onchange="temp_slider_energy_min(this.value)" min="0.01" max="100" step="0.1">
     </div>
    <div class="doublecolumn">
      <figure style = "width:100%; height:280px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "Figure_energy_minima_and_temp">
    <div id = "energy_minima_and_temp_id" style="position:absolute; left:0px; top:0px"></div>
    </figure>
    <script type = "text/javascript" src = "energy_minima_and_temp.js" ></script>
    </div>
  </div>
  <div class="triplecolumn"><p class="figure-text"> This figure illustrates two local energy minima that contain a different MNIST number as their minimum energy. If we are at zero temperature any higher energy configuration would equilibrate to the closest local minimum and the only configurations that can be retrieved are the ones at the minimum energy. If we allow a certain temperature (blue shaded area) we can also retrieve configurations with a slightly higher energy. This eventually makes the system to generalize training data.</p>  </div>
    <p class="figure-text"><b>Influence of Temperature on Configurations: </b>After training local energy minima correspond to configurations of the training set. At zero temperature configurations with higher energy will equilibrate to the minimum energy configuration. For non zero temperature higher energy configurations can be occupied with a certain probability here shown as a shaded blue area around the minima.</p>


<p>
One possible improvement to this approach is to consider models where the local update rule is even better at finding low-energy configurations. Since the change in energy <d-math>\Delta E_i = 2\sigma_i(b_i+\sum_j w_{ij}\sigma_j)</d-math> for particle <d-math>i</d-math> depends on the states of all other particles, each time a state <d-math>\sigma_i </d-math> is updated to <d-math>-\sigma_i </d-math>, it spreads out to all other particles, changing their update rules. For an RBM, however, this is not exactly the case. The energy change in a visible node <d-math> v_i </d-math>,  <d-math>\Delta E_i = 2v_i(b_i+\sum_j w_{ij}h_j)</d-math> does not depend on any of the other visible nodes, so when one of them is flipped, all other visible nodes are unaffected. This makes the local rule update more effective because we can essentially treat the entire collection of visible nodes as a single entity. A similar statement holds for updating hidden nodes when the visible ones are fixed. In fact, the lack of intralayer interactions in an RBM implies that the conditional probabilities <d-math>P(v|h)</d-math> and <d-math>P(h|v)</d-math> factorize: </p>
 <p>
 <d-math block>
  \begin{aligned}
  P(v|h) &= \prod_i P(v_i|h)\\
  P(h|v) &= \prod_j P(h_j|v).
  \end{aligned}
  </d-math>
</p>
<p>
Moreover, because of the independence between nodes in the same layer, the individual conditional probabilities can be calculated analytically, and are given by: <d-cite key="fischer2012introduction"></d-cite></p>
<p>
<d-math block>
  \begin{aligned}
  P(v_i=1|h) &= \frac{1}{1+e^{-\Delta E_i/T}}\\
  P(h_j=1|v) &= \frac{1}{1+e^{-\Delta E_j/T}}.
  \end{aligned}
</d-math>
</p>
<p>
Analogous to the energy difference of flipping a visible node, for a hidden node <d-math>h_j </d-math> the energy difference becomes <d-math>\Delta E_j = 2 h_j( \sum_i w_{i,j} v_i + c_j)</d-math>.
Note that <d-math>P(v_i=-1|h)</d-math> and <d-math>P(h_j=-1|v)</d-math> follow from these equations. This permits a new sampling strategy. Fixing the hidden nodes, we sample from the conditional distribution <d-math>P(v|h) = \prod_i P(v_i|h)</d-math> by independently sampling the state of each particle from its distribution <d-math>P(v_i|h)</d-math>. Afterwards, the visible nodes are fixed and the same method is applied to the hidden units. By repeating this process several times, the resulting states will be approximately sampled from the system's Boltzmann distribution. This algorithm is known as <em>Gibbs sampling</em>, and it is the method typically used in practice to sample RBMs <d-cite key="carreira2005contrastive"></d-cite>. 
</p>

<!-- Second Section  4  -->
  <a class="marker" href="#section-4" id="section-4"><span>Training</span></a>
  <h2 id="training">Training</h2>

<p>
Significant progress has been made so far: we have understood how probabilistic models can be built from physical systems at thermal equilibrium, we have identified useful architectures for these systems, and have developed algorithms to sample from their Boltzmann distributions. A final challenge remains: how can we train these systems to perform specific tasks? Training in this sense means identifying the parameters of the energy function that give rise to the desired probability distribution. Searching for inspiration in physics has so far proved fruitful; let's try that again.
</p>

<p>
The identification of binary variables with physical spins, as was done above, is a common connection between energy-based models and physical systems. However, for illustrative purposes, let us consider a simple physical system which may be more familiar: a mass attached to a spring with the spring constant <d-math>k</d-math>, which experiences a position-dependent force <d-math>F(y)= -k y</d-math>. If this mass is placed in a gravitational field, it experiences a constant external force <d-math>F_g = mg </d-math>, where <d-math>g</d-math> is the acceleration due to gravity. The mass is only in equilibrium at the precise position where these two forces balance, i.e., when <d-math>F(y)+ F_g = 0</d-math>. Now suppose we have a simple model with an energy-function <d-math>E(x) = \theta f(x)</d-math>. As discussed previously, the expectation value <d-math> f(x) \rangle </d-math> is a sufficient statistic of the Boltzmann distribution <d-math> P(x)=\frac{1}{Z}\exp[-E(x)/T]. </d-math> Knowing this expectation uniquely fixes the parameters <d-math> \theta</d-math> and therefore also the distribution itself. Now suppose that our goal is to train an energy-based model to reproduce the statistics of a dataset, specified as a set of configurations
<d-math>(x^{(1)}, x^{(2)}, \ldots, x^{(N)})</d-math>. Training the model is equivalent to identifying the parameter <d-math>\theta</d-math> such that the sufficient statistics of the model coincide with those of the data, i.e., <d-math> \langle f(x) \rangle_{\text{model}} = \langle f(x) \rangle_{\text{data}}</d-math>. Since the expectation over the model distribution depends on <d-math>\theta</d-math>, we can write <d-math> \langle f(x) \rangle_{\text{model}} = -F(\theta) </d-math>, whereas the expectation over data is a constant, <d-math> \langle f(x) \rangle_{\text{data}} = F_g </d-math>. Interpreting these as generalized forces acting in opposite directions, and the parameter <d-math>\theta</d-math> as a generalized position, we conclude that the model is trained when the position <d-math>\theta</d-math> is such that the forces are in equilibrium: <d-math>F(\theta)+F_g=0</d-math>. 
</p>

<p>
If forces are out of equilibrium, for instance if the pull of gravity outweighs the restoring force of the spring, objects accelerate and change position. For an object starting at rest, the displacement due to an inbalance of forces is
</p>
<p> 
  <d-math block> 
    \theta \rightarrow \theta + \eta(\langle f(x) \rangle_{\text{data}}-\langle f(x) \rangle_{\text{model}}),
  </d-math>
  </p>
  <p>
where <d-math>\eta>0</d-math> is a constant that depends on the object's mass and the time for which the forces act, which we assume to be small. The first force, <d-math>\langle f(x) \rangle_{\text{data}}</d-math>, can be thought of as an external force provided by an outside system, like the Earth's gravitational field. In this case, the external system is actually a set of training data. These data provide a constant downward "pull" towards preferred configurations which are exemplified in the dataset. 
The second force,  <d-math>\langle f(x) \rangle_{\text{model}}</d-math>, represents the system's natural preference for certain configurations. This provides a certain kind of internal "lift", working against the downward pull of the training data. Crucially, in the presence of two competing forces acting in different directions, the resulting displacement causes the object to move to a position that reduces the inbalance of forces. For example, if <d-math> F_g> ky </d-math> in our previous example, the spring is pulled downwards to a new position <d-math>y'>y</d-math> that increases the force due the spring, so both forces are closer to balance. Overall, by repeatedly applying the update rule above for sufficiently small step sizes, we can find a parameter value that balances the two ficticious forces, leading to a trained model. Let's now explore how to develop this physical intuition into concrete training strategies for energy-based models.
</p>


  <div class="row">
  <div class="column">
  <p class="slider-label-text">Training progress</p>
  <p id="temperature_slider" class="slider-label-number"></p>
    <input id="temp_slider_id" type="range" oninput="slider_fct_image_energies(this.value)" min="0" max="100" step="1.0">
  </div>
    <div class="doublecolumn">
    <p class="figure-text">add some text here</p>
    <figure style = "width:700px; height:500px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "image_energies">
  <div id = "image_energies_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  </div>
  </div>
  <script type = "text/javascript" src = "image_energies_script_new.js" ></script>
  <div class="triplecolumn"><p class="figure-text"><b>The purpose of training </b>is to find the model parameters that make the wanted configurations low energy. The model parameters are initiallized randomly and therefore the configuration's energies are also random. During the training we want to minimize the energies of the images we want to learn where at the same time we maximize the energies of configurations we don't want. This figure illustrates the evolution of energies over the training progress.</p></div>

<!-- Second Section  4.1  -->
  <a class="marker" href="#section-4.1" id="section-4.1"><span>Training Hopfield networks</span></a>
  <h3 id="hopfield-networks-training">Training Hopfield networks</h3>

  <p>Training a Hopfield network is equivalent to identifying parameters such that the ground states of the energy function are the configurations of the input data. Suppose we are given a single <d-math>n</d-math>-dimensional data vector <d-math>\sigma^{(1)}</d-math>. There is a straightforward way to ensure it is a ground state: set all <d-math>b_i=0</d-math> and fix <d-math>w_{ij}=-\sigma^{(1)}_i\sigma^{(1)}_j</d-math>. The energy function is then <d-math>E(\sigma)=-\langle \sigma^{(1)}, \sigma \rangle</d-math>, which attains its minimum when the inner product is maximized, i.e, when <d-math>\sigma=\sigma^{(1)}</d-math>. For more data points, we follow a similar rule, this time setting the interaction parameters to <d-math>w_{ij}=-\frac{1}{N}\sum_{k=1}^N\sigma^{(k)}_i\sigma^{(k)}_j=\langle \sigma_i \sigma_j \rangle_{\text{data}}</d-math>. Note the appearance of the ficticious force we studied before. This strategy is known as <em>Hebbian learning</em> and works best when all data vectors are mutually orthogonal (or nearly so), in which case all data points are local minima of the energy function. If not, so-called spurious minima can appear that do not correspond to data vectors, leading to erroneous memory retrieval.

  </p>

  <p>To address the issue of spurious minima, the concept of “unlearning” was introduced to improve the performance of Hopfield networks <d-cite key="hopfield_unlearning_1983"></d-cite>. For unlearning, we expand the rule by setting </p>
  <p> 
 <d-math block>
 \begin{aligned}
  w_{i,j} &= \langle \sigma_i \sigma_j \rangle_{\text{data}} - \epsilon -\frac{1}{N}\sum_{k=1}^N\tilde{\sigma}^{(k)}_i\tilde{\sigma^{(k)}_j}\\
  &= \langle \sigma_i \sigma_j \rangle_{\text{data}} - \epsilon \langle \sigma_i \sigma_j \rangle_{\text{model}},
  \end{aligned}
  <d-math>
</p>
<p>
where the model equilibrium states <d-math>\{\tilde{\sigma}^{(1)},\ldots, \tilde{\sigma}^{(N)} \}</d-math> 
are each obtained by choosing a random initial configuration and then using the sampling algorithms described in the previous section to find equilibrium states. 
During sampling, the weights of the model are set according to the Hebbian rule. The addition of this expectation value over model configurations plays against the first term of the update rule <d-math>\langle \sigma_i \sigma_j \rangle_{\text{Data}}</d-math>, 
leading to an increase in energy of all minima, including spurious ones. 
By appropriately selecting the value of <d-math>\epsilon</d-math>, this increase in energy can lead to the disappearance of spurious minima, which are thus "unlearned". </p>

  <div class="row">
  <div class="triplecolumn">
    <input id="learn_button_id" type="button" value="Learning step" onclick="learn_phase()">
    <input id="learn_button_id" type="button" value="Unlearning step" onclick="unlearn_phase()">
    <input id="learn_button_id" type="button" value="Reinitialize" onclick="reinitialize_phase()">
  </div>
  <div class="triplecolumn">
    <figure style = "width:100%; height:400px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "energy_landscape">
  <div id = "energy_landscape_figure_id" style="width:100%, height:100%, position:absolute; left:0px; top:0px"></div>
  </figure>
  <div class="triplecolumn">
	  <p class ="figure-text"><b>Learning and Unlearning: </b> A randomly initialized spin system has a energy landscape with many different local minima, which in physics is often referred to as the spinglass phase <d-cite key="amit1985spin"> </d-cite>. Learning a certain configuration (red dots) in energy based models means we need to decrease its energy. But only by learning a configuration we do not get rid of spurious minima that contain unwanted configuration (yellow). They have to be unlearned.</p>
  </div> 
  <script type = "text/javascript" src = "energy_landscape.js" ></script>
  </div>
  </div>



  <div class="row">
  <div class="column">
  <p id="weight_slider_text" class="slider-label-text">Weight</p>
  <p id="weight_slider_value" class="slider-label-number"></p>
    <input id="weight_slider_id" value=0 type="range" onchange="slider_fct_RBM(this.value,'')" min="-2" max="2" step="0.01">
  <p id="bias_slider_text" class="slider-label-text">Bias</p>
  <p id="bias_slider_value" class="slider-label-number" width="50%"></p>
    <input id="bias_slider_id" value=0 type="range" oninput="slider_bias_just_text(this.value,'')" onchange="slider_bias_fct_RBM(this.value,'')" min="-2" max="2" step="0.01">
  <div>
  <input type="checkbox" onclick="change_layout_hidden('')" id="hidden_check" name="hidden_check"
         checked>
  <label for="hidden_check">Allow hidden units</label>
  </div>
  <div>
  <input type="checkbox" onclick="change_layout_restricted('')" id="restricted_check" name="restricted_check">
  <label for="restricted_check">Make it restricted</label>
  </div>
  </div>
  <div class="doublecolumn">
    <figure style = "width:100%; height:320px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "RBM_complete">
  <div id = "RBM_complete_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  <script type = "text/javascript" src = "RBM_complete_new.js" ></script>
  </div>
  </div>
  <div class="caption-triplecolumn"><p class="figure-text"> Try to make the "Bars and Stripes" configurations maximally likely.

 <div class="row">
  <div class="column">
  <p id="weight_slider_text_XOR" class="slider-label-text">Weight</p>
  <p id="weight_slider_value_XOR" class="slider-label-number"></p>
    <input id="weight_slider_id_XOR" value=0 type="range" onchange="slider_fct_RBM(this.value,'_XOR')" min="-2" max="2" step="0.01">
  <p id="bias_slider_text_XOR" class="slider-label-text">Bias</p>
  <p id="bias_slider_value_XOR" class="slider-label-number" width="50%"></p>
    <input id="bias_slider_id_XOR" value=0 type="range" oninput="slider_bias_just_text(this.value,'_XOR')" onchange="slider_bias_fct_RBM(this.value,'_XOR')" min="-2" max="2" step="0.01">
  <div>
  <input type="checkbox" onclick="change_layout_hidden('_XOR')" id="hidden_check_XOR" name="hidden_check_XOR"
         checked>
  <label for="hidden_check">Allow hidden units</label>
  </div>
  <div>
  <input type="checkbox" onclick="change_layout_restricted('_XOR')" id="restricted_check_XOR" name="restricted_check_XOR">
  <label for="restricted_check">Make it restricted</label>
  </div>
  </div>
  <div class="doublecolumn">
    <figure style = "width:100%; height:320px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "RBM_complete_XOR">
  <div id = "RBM_complete_id_XOR" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  <script type = "text/javascript" src = "RBM_complete_XOR.js" ></script>
  </div>
  </div>
  <div class="caption-triplecolumn"><p class="figure-text"> Try to make the "Bars and Stripes" configurations maximally likely.
  For this you have to adjust the weights and biases by clicking on the conections and the biases and adjust them with the slider.
  The color of the nodes indicates in which direction the spins are. This can help you to find the weights for a certain configurations and make its energy high or low. Remember that the wanted configurations should be low energy. And all the others should be higher energy. A way to find the solution for this task is to set the spins of the model to a B&S configuration and make its energy as small as possible. Then change to another B&S configuration and repeat. You will figure out eventually how you "learn" the weights. If you want to have a working solution, click the solution button.</p></div>
	  

  <a class="marker" href="#section-4.3" id="section-4.3"><span>Training Boltzmann machines</span></a>
  <h3 id="r-bm-and-sampling">Training Boltzmann machines</h3>

  <p>Hebbian learning and unlearning techniques face a challenge when applied to Boltzmann machines. Since only the visible nodes encode data, it is not clear how to assign values to the hidden nodes. To address this, the Hebbian learning rule can be upgraded to an <em>update</em> rule that iteratively improves the weights with each step. By doing so, we allow the hidden nodes to "move" together with the visible ones, leading to training of the complete model. Starting from some initial value <d-math>w_{ij}</d-math>, a weight is updated to
  </p>
  <p>
  <d-math block>
    w_{ij}\rightarrow w_{ij}+\eta(\langle \sigma_i \sigma_j \rangle_{\text{data}} - \epsilon \langle \sigma_i \sigma_j \rangle_{\text{model}}),
</d-math>
</p>
<p>
  where <d-math>\eta>0</d-math> is a small <em>learning rate</em>. Whenever <d-math>\sigma_i\sigma_j=v_ih_j</d-math> or <d-math>\sigma_i\sigma_j=h_ih_j</d-math>, averages over the data are taken by fixing the visible units to the data and then using Gibbs sampling to obtain values for the hidden nodes. In the specific case of an RBM, when setting <d-math>\epsilon=1</d-math>, the update rule takes the form
  </p>
  <p>
   <d-math block>
    w_{ij}\rightarrow w_{ij}+\eta(\langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}}).
  </d-math>
  </p>
  <p>
This rule is known as the <em>contrastive divergence</em> formula for training RBMs. It is usually derived from the gradient of the Kullback-Leibler divergence between the model and data distributions, but our analysis provides deeper intuition about its properties. On the one hand, the Hebbian learning term <d-math> \langle v_i h_j\rangle_{\text{Data}}</d-math>, usually referred to as the <em>positive phase</em>, decreases the energy of the configurations of the training data, while the unlearning term <d-math> \langle v_i h_j\rangle_{\text{Model}}</d-math>, often referred to as the <em>negative phase</em>, increases the energy of all the configurations that are at equilibrium. From this perspective, the role of contrastive divergence is to gradually shape the energy function of the model until all low energy states of the model correspond to data points. On the other hand, viewing the weights as generalized positions, we interpret the term <d-math>\Delta w_{ij}:=\langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}}</d-math> as a net force originating from two competing forces, a fixed external one due data and an internal one due to specific generalized positions of the system. When the forces are not balanced, this causes a shift in position. Depending on which force is strongest, the weight will increase or decrease in value, with the effect of bringing these forces closer to balance. Since each shift in weight propagates across the entire model affecting all forces, the strategy of the contrastive divergence training algorithm is to perform many small shifts (small learning rate) until all forces are balanced and the model is trained.
  </p>


  

  <div class="row">
  <div class="triplecolumn">
    <figure style = "width:100%; height:500px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "architecture_Hopfield">
  <div id = "image_equilibration_id" style="position:absolute; left:0px; top:0px"></div>
  </figure>
  </div>
  <div class="triplecolumn">
    <p class="figure-text"> One  <input id="equilibrate_button_id" type="button" value="Equilibration step" onclick="equilibration_step_rbm()"> brings the damaged initial configuration closer to a low energy configuration. The hidden and the visible layers are updated one after another. </p><p class="figure-text"><b>Sampling through equilibration:</b> This figure shows a restricted Boltzmann machine, where the visible units are initialized with a partially damaged MNIST digit and the hidden units are initiallized all in the same direction. In an RBM, the equilibration is sequentially, where the hidden and the visible layer is sampled one after another dependend of the configuration of the other layer. The energy during the equilibration process will go to a lower value.</p>
  </div>
  </div>
  <script type = "text/javascript" src = "image_equilibration_script.js" ></script>

  <div class="triplecolumn"></div>

<!-- Second Section  4  -->
  <a class="marker" href="#section-5" id="section-5"><span>Conclusion</span></a>
  <h2 id="conclusion">Conclusion</h2>

  <p><b>PW: we should finish the conclusions with a paragraph on how these insights will help develop new energy-based models, e.g., new QBMs, continuous-valued stuff, what-not.</b></p>

</d-article>

<d-appendix>

  <h3>Contributions</h3>
  <p>Some text describing who did what.</p>
  <h3>Reviewers</h3>
  <p>Some text with links describing who reviewed the article.</p>
<d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
  <d-bibliography src="bibliography.bib"></d-bibliography>

<!-- <d-bibliography>
    <script type="text/bibtex">
      @article{gregor2015draw,
        title={DRAW: A recurrent neural network for image generation},
        author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
        journal={arXiv preprint arXiv:1502.04623},
        year={2015},
        url ={https://arxiv.org/pdf/1502.04623.pdf}
      }
      @article{mercier2011humans,
        title={Why do humans reason? Arguments for an argumentative theory},
        author={Mercier, Hugo and Sperber, Dan},
        journal={Behavioral and brain sciences},
        volume={34},
        number={02},
        pages={57--74},
        year={2011},
        publisher={Cambridge Univ Press},
        doi={10.1017/S0140525X10000968}
      }
    </script>
  </d-bibliography> -->

  <distill-appendix> </distill-appendix>

</d-appendix>

<distill-footer></distill-footer>

</body>
