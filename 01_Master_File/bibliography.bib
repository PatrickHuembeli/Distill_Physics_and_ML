@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/79/8/2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	pages = {2554--2558},
	number = {8},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Hopfield, J. J.},
	urldate = {2019-05-14},
	date = {1982-04-01},
	langid = {english},
	pmid = {6953413},
	file = {Full Text PDF:/Users/phuembeli/Zotero/storage/QZMJXHZQ/Hopfield - 1982 - Neural networks and physical systems with emergent.pdf:application/pdf;Snapshot:/Users/phuembeli/Zotero/storage/PB7FKUYI/2554.html:text/html}
}

@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{salakhutdinov2007restricted,
  title={Restricted Boltzmann machines for collaborative filtering},
  author={Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={791--798},
  year={2007},
  organization={ACM}
}


@article{le2010deep,
  title={Deep belief networks are compact universal approximators},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  journal={Neural computation},
  volume={22},
  number={8},
  pages={2192--2207},
  year={2010},
  publisher={MIT Press}
}

@article{hopfield_unlearning_1983,
	title = {"Unlearning" has a stabilizing effect in collective memories},
	volume = {304},
	rights = {1983 Nature Publishing Group},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/304158a0},
	doi = {10.1038/304158a0},
	abstract = {Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an ‘unlearning’ process. We have independently carried out mathematical and computer modelling of learning and ‘unlearning’ in a collective neural network of 30–1,000 neurones. The model network has a content-addressable memory or ‘associative memory’ which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an ‘unlearning’ process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of ‘unlearning’ in rapid eye movement ({REM}) sleep.},
	pages = {158},
	number = {5922},
	journaltitle = {Nature},
	author = {Hopfield, J. J. and Feinstein, D. I. and Palmer, R. G.},
	urldate = {2019-05-14},
	date = {1983-07},
	file = {Snapshot:/Users/phuembeli/Zotero/storage/NEK3XJ7X/304158a0.html:text/html}
}


@article{amit1985spin,
  title={Spin-glass models of neural networks},
  author={Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={32},
  number={2},
  pages={1007},
  year={1985},
  publisher={APS}
}

@article{cipra1987introduction,
  title={An introduction to the Ising model},
  author={Cipra, Barry A},
  journal={The American Mathematical Monthly},
  volume={94},
  number={10},
  pages={937--959},
  year={1987},
  publisher={Taylor \& Francis}
}

@inproceedings{li2013learning,
  title={Learning discriminative sufficient statistics score space for classification},
  author={Li, Xiong and Wang, Bin and Liu, Yuncai and Lee, Tai Sing},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={49--64},
  year={2013},
  organization={Springer}
}

@article{le2008representational,
  title={Representational power of restricted Boltzmann machines and deep belief networks},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  journal={Neural computation},
  volume={20},
  number={6},
  pages={1631--1649},
  year={2008},
  publisher={MIT Press}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@book{Rojas_1996_NNS,
 author = {Rojas, Ra\'{u}l},
 title = {Neural Networks: A Systematic Introduction},
 year = {1996},
 isbn = {3-540-60505-3},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@inproceedings{fischer2012introduction,
  title={An introduction to restricted Boltzmann machines},
  author={Fischer, Asja and Igel, Christian},
  booktitle={Iberoamerican Congress on Pattern Recognition},
  pages={14--36},
  year={2012},
  organization={Springer}
}

@inproceedings{carreira2005contrastive,
  title={On contrastive divergence learning.},
  author={Carreira-Perpinan, Miguel A and Hinton, Geoffrey E},
  booktitle={Aistats},
  volume={10},
  pages={33--40},
  year={2005},
  organization={Citeseer}
}

@article{gregor2015draw,
         title={DRAW: A recurrent neural network for image generation},
         author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
         journal={arXiv preprint arXiv:1502.04623},
         year={2015},
         url ={https://arxiv.org/pdf/1502.04623.pdf}
       }
@article{mercier2011humans,
         title={Why do humans reason? Arguments for an argumentative theory},
         author={Mercier, Hugo and Sperber, Dan},
         journal={Behavioral and brain sciences},
         volume={34},
         number={02},
         pages={57--74},
         year={2011},
         publisher={Cambridge Univ Press},
         doi={10.1017/S0140525X10000968}
       }
@article{du2019implicit,
       author      = {Yilun Du and Igor Mordatch},
       title       = {Implicit Generation and Generalization in Energy-Based Models},
       archiveprefix = {arXiv},
       eprint      = {1903.08689},
       year        = {2019}
       }
 @article{nijkamp2019on,
   author      = {Erik Nijkamp and Mitch Hill and Tian Han and Song-Chun Zhu and Ying Nian Wu},
   title       = {On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based
   Models},
   archiveprefix = {arXiv},
   eprint      = {1903.12370},
   year        = {2019},
   abstract    = {This study investigates the effects of Markov Chain Monte Carlo (MCMC)
 sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is
 restricted to the family of unnormalized probability densities for which the
 negative log density (or energy function) is a ConvNet. In general, we find
 that many of the techniques used to stabilize training in previous studies can
 have the opposite effect. Stable ML learning with a ConvNet potential can be
 achieved with only a few hyper-parameters and no regularization. Using this
 minimal framework, we identify a variety of ML learning outcomes that depend on
 the implementation of MCMC sampling.
   On one hand, we show that it is easy to train an energy-based model which can
 sample realistic images with short-run Langevin. ML can be effective and stable
 even when MCMC samples have much higher energy than true steady-state samples
 throughout training. Based on this insight, we introduce an ML method with
 purely noise-initialized MCMC, high-quality short-run synthesis, and the same
 budget as ML with informative MCMC initialization such as CD or PCD. Unlike
 previous models, our model can obtain realistic high-diversity samples from a
 noise signal after training with no auxiliary networks.
   On the other hand, ConvNet potentials learned with highly non-convergent MCMC
 do not have a valid steady-state and cannot be considered approximate
 unnormalized densities of the training data because long-run MCMC samples
 differ greatly from observed images. We show that it is much harder to train a
 ConvNet potential to learn a steady-state over realistic images. To our
 knowledge, long-run MCMC samples of all previous models lose the realism of
 short-run samples. With correct tuning of Langevin noise, we train the first
 ConvNet potentials for which long-run and steady-state MCMC samples are
 realistic images.},
   timestamp={2019.08.09}
 }
 @article{orus2014practical,
         author = {Or{\'u}s, Rom{\'a}n},
         doi = {10.1016/j.aop.2014.06.013},
         journal = {Annals of Physics},
         pages = {117--158},
         publisher = {Elsevier {BV}},
         timestamp = {2019.08.09},
         title = {A practical introduction to tensor networks: Matrix product states and projected entangled pair states},
         volume = {349},
         year = {2014}
 }
 @article{stoudenmire2016supervised,
         Author = {Stoudenmire, E. Miles and Schwab, David J.},
         eprint = {1605.05775},
         journal = {Advances in Neural Information Processing Systems},
         pages = {4799?4807},
         timestamp = {2016.10.02},
         title = {Supervised Learning with Tensor Networks},
         volume = {29},
         year = {2016}
 }
 @article{stoudenmire2018learning,
         abstract = {Inspired by coarse-graining approaches used in physics, we show how similar
 algorithms can be adapted for data. The resulting algorithms are based on
 layered tree tensor networks and scale linearly with both the dimension of the
 input and the training set size. Computing most of the layers with an
 unsupervised algorithm, then optimizing just the top layer for supervised
 classification of the MNIST and fashion-MNIST data sets gives very good
 results. We also discuss mixing a prior guess for supervised weights together
 with an unsupervised representation of the data, yielding a smaller number of
 features nevertheless able to give good performance.},
         archiveprefix = {arXiv},
         author = {Stoudenmire, E. Miles},
         eprint = {1801.00315},
         journal = {Quantum Science and Technology},
         doi = {10.1088/2058-9565/aaba1a},
         timestamp = {2018.05.21},
         title = {Learning Relevant Features of Data with Multi-scale Tensor Networks},
         volume = {3},
         year = {2018},
         pages = {034003}
 }
 @article{huggins2019towards,
   doi = {10.1088/2058-9565/aaea94},
   url = {https://doi.org/10.1088%2F2058-9565%2Faaea94},
   year = {2019},
   publisher = {{IOP} Publishing},
   volume = {4},
   number = {2},
   pages = {024001},
   author = {William Huggins and Piyush Patil and Bradley Mitchell and K Birgitta Whaley and E Miles Stoudenmire},
   title = {Towards quantum machine learning with tensor networks},
   journal = {Quantum Science and Technology},
   archiveprefix = {arXiv},
   eprint      = {1803.11537},
   abstract    = {Machine learning is a promising application of quantum computing, but
 challenges remain as near-term devices will have a limited number of physical
 qubits and high error rates. Motivated by the usefulness of tensor networks for
 machine learning in the classical context, we propose quantum computing
 approaches to both discriminative and generative learning, with circuits based
 on tree and matrix product state tensor networks that could have benefits for
 near-term devices. The result is a unified framework where classical and
 quantum computing can benefit from the same theoretical and algorithmic
 developments, and the same model can be trained classically then transferred to
 the quantum setting for additional optimization. Tensor network circuits can
 also provide qubit-efficient schemes where, depending on the architecture, the
 number of physical qubits required scales only logarithmically with, or
 independently of the input or output data sizes. We demonstrate our proposals
 with numerical experiments, training a discriminative model to perform
 handwriting recognition using a optimization procedure that could be carried
 out on quantum hardware, and testing the noise resilience of the trained model.},
         timestamp = {2018.05.21},
 }
 @article{roberts2019tensornetwork,
   author      = {Chase Roberts and Ashley Milsted and Martin Ganahl and Adam Zalcman and Bruce Fontaine and Yijian Zou and Jack Hidary and Guifre Vidal and Stefan Leichenauer},
   title       = {TensorNetwork: A Library for Physics and Machine Learning},
   archiveprefix = {arXiv},
   eprint      = {1905.01330},
   year        = {2019},
   abstract    = {TensorNetwork is an open source library for implementing tensor network
 algorithms. Tensor networks are sparse data structures originally designed for
 simulating quantum many-body physics, but are currently also applied in a
 number of other research areas, including machine learning. We demonstrate the
 use of the API with applications both physics and machine learning, with
 details appearing in companion papers.},
   timestamp={2019.08.09}
 }
 @article{efthymiou2019tensornetwork,
   author      = {Stavros Efthymiou and Jack Hidary and Stefan Leichenauer},
   title       = {TensorNetwork for Machine Learning},
   archiveprefix = {arXiv},
   eprint      = {1906.06329},
   year        = {2019},
   abstract    = {We demonstrate the use of tensor networks for image classification with the
 TensorNetwork open source library. We explain in detail the encoding of image
 data into a matrix product state form, and describe how to contract the network
 in a way that is parallelizable and well-suited to automatic gradients for
 optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
 find out-of-the-box performance of 98% and 88% accuracy, respectively, using
 the same tensor network architecture. The TensorNetwork library allows us to
 seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
 improvement in computational speed using a GPU.},
   timestamp={2019.08.09}
 }
 @article{liu2019machine,
   doi = {10.1088/1367-2630/ab31ef},
   url = {https://doi.org/10.1088%2F1367-2630%2Fab31ef},
   year = {2019},
   publisher = {{IOP} Publishing},
   volume = {21},
   number = {7},
   pages = {073059},
   author = {Ding Liu and Shi-Ju Ran and Peter Wittek and Cheng Peng and Raul Bl{\'{a}}zquez Garc{\'{\i}}a and Gang Su and Maciej Lewenstein},
   title = {Machine learning by unitary tensor network of hierarchical tree structure},
   journal = {New Journal of Physics},
   archiveprefix = {arXiv},
   eprint      = {1710.04833},
   abstract    = {The resemblance between the methods used in quantum-many body physics and in
 machine learning has drawn considerable attention. In particular, tensor
 networks (TNs) and deep learning architectures bear striking similarities to
 the extent that TNs can be used for machine learning. Previous results used
 one-dimensional TNs in image recognition, showing limited scalability and
 flexibilities. In this work, we train two-dimensional hierarchical TNs to solve
 image recognition problems, using a training algorithm derived from the
 multi-scale entanglement renormalization ansatz. This approach introduces
 mathematical connections among quantum many-body physics, quantum information
 theory, and machine learning. While keeping the TN unitary in the training
 phase, TN states are defined, which encode classes of images into quantum
 many-body states. We study the quantum features of the TN states, including
 quantum entanglement and fidelity. We find these quantities could be properties
 that characterize the image classes, as well as the machine learning tasks.},
   timestamp={2019.08.09}
 }
 @article{biamonte2017quantum,
         abstract = {Recent progress implies that a crossover between machine learning and quantum
 information processing benefits both fields. Traditional machine learning has
 dramatically improved the benchmarking and control of experimental quantum
 computing systems, including adaptive quantum phase estimation and designing
 quantum computing gates. On the other hand, quantum mechanics offers
 tantalizing prospects to enhance machine learning, ranging from reduced
 computational complexity to improved generalization performance. The most
 notable examples include quantum enhanced algorithms for principal component
 analysis, quantum support vector machines, and quantum Boltzmann machines.
 Progress has been rapid, fostered by demonstrations of midsized quantum
 optimizers which are predicted to soon outperform their classical counterparts.
 Further, we are witnessing the emergence of a physical theory pinpointing the
 fundamental and natural limitations of learning. Here we survey the cutting
 edge of this merger and list several open problems.},
         archiveprefix = {arXiv},
         author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
         doi = {10.1038/nature23474},
         eprint = {1611.09347},
         journal = {Nature},
         number = {7671},
         pages = {195--202},
         publisher = {Springer Nature},
         timestamp = {2016.11.29},
         title = {Quantum Machine Learning},
         volume = {549},
         year = {2017},
         year2 = {2016}
 }
 @article{amin2018quantum,
         abstract = {Inspired by the success of Boltzmann Machines based on classical Boltzmann
 distribution, we propose a new machine learning approach based on quantum
 Boltzmann distribution of a transverse-field Ising Hamiltonian. Due to the
 non-commutative nature of quantum mechanics, the training process of the
 Quantum Boltzmann Machine (QBM) can become nontrivial. We circumvent the
 problem by introducing bounds on the quantum probabilities. This allows us to
 train the QBM efficiently by sampling. We show examples of QBM training with
 and without the bound, using exact diagonalization, and compare the results
 with classical Boltzmann training. We also discuss the possibility of using
 quantum annealing processors like D-Wave for QBM training and application.},
         archiveprefix = {arXiv},
         author = {Amin, Mohammad H. and Andriyash, Evgeny and Rolfe, Jason and Kulchytskyy, Bohdan and Melko, Roger},
         doi = {10.1103/physrevx.8.021050},
         eprint = {1601.02036},
         journal = {Physical Review X},
         keywords = {qml; quantum machine learning},
         month = jan,
         number = {2},
         pages = {021050},
         publisher = {American Physical Society ({APS})},
         timestamp = {2016.02.25},
         title = {Quantum {Boltzmann} Machine},
         volume = {8},
         year = {2018},
         year2 = {2016}
 }
 @article{preskill2018quantum,
         abstract = {Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the
 near future. Quantum computers with 50-100 qubits may be able to perform tasks
 which surpass the capabilities of today's classical digital computers, but
 noise in quantum gates will limit the size of quantum circuits that can be
 executed reliably. NISQ devices will be useful tools for exploring many-body
 quantum physics, and may have other useful applications, but the 100-qubit
 quantum computer will not change the world right away --- we should regard it
 as a significant step toward the more powerful quantum technologies of the
 future. Quantum technologists should continue to strive for more accurate
 quantum gates and, eventually, fully fault-tolerant quantum computing.},
         archiveprefix = {arXiv},
         author = {Preskill, John},
         doi = {10.22331/q-2018-08-06-79},
         eprint = {1801.00862},
         journal = {Quantum},
         pages = {79},
         timestamp = {2018.02.15},
         title = {Quantum Computing in the {NISQ} era and beyond},
         volume = {2},
         year = {2018}
 }
ok{Wittek2014Quantum,
         address = {New York, NY, USA},
         author = {Wittek, Peter},
         month = aug,
         publisher = {Elsevier},
         timestamp = {2015.03.31},
         title = {Quantum Machine Learning: What Quantum Computing Means to Data Mining},
         year = {2014}
 }
 @article{khoshaman2018quantum,
   doi = {10.1088/2058-9565/aada1f},
   url = {https://doi.org/10.1088%2F2058-9565%2Faada1f},
   year = {2018},
   publisher = {{IOP} Publishing},
   volume = {4},
   number = {1},
   pages = {014001},
   author = {Amir Khoshaman and Walter Vinci and Brandon Denis and Evgeny Andriyash and Hossein Sadeghi and Mohammad H Amin},
   title = {Quantum variational autoencoder},
   journal = {Quantum Science and Technology},
   archiveprefix = {arXiv},
   eprint      = {1802.05779},
   abstract    = {Variational autoencoders (VAEs) are powerful generative models with the
 salient ability to perform inference. Here, we introduce a quantum variational
 autoencoder (QVAE): a VAE whose latent generative process is implemented as a
 quantum Boltzmann machine (QBM). We show that our model can be trained
 end-to-end by maximizing a well-defined loss-function: a 'quantum' lower-bound
 to a variational approximation of the log-likelihood. We use quantum Monte
 Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To
 achieve the best performance, we first create a VAE platform with discrete
 latent space generated by a restricted Boltzmann machine (RBM). Our model
 achieves state-of-the-art performance on the MNIST dataset when compared
 against similar approaches that only involve discrete variables in the
 generative process. We consider QVAEs with a smaller number of latent units to
 be able to perform QMC simulations, which are computationally expensive. We
 show that QVAEs can be trained effectively in regimes where quantum effects are
 relevant despite training via the quantum bound. Our findings open the way to
 the use of quantum computers to train QVAEs to achieve competitive performance
 for generative models. Placing a QBM in the latent space of a VAE leverages the
 full potential of current and next-generation quantum computers as sampling
 devices.},
         timestamp = {2018.02.20},
 }
 
@article{melko2019restricted,
  title={Restricted Boltzmann machines in quantum physics},
  author={Melko, Roger G and Carleo, Giuseppe and Carrasquilla, Juan and Cirac, J Ignacio},
  journal={Nature Physics},
  volume={15},
  number={9},
  pages={887--892},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{du2019implicit,
  title={Implicit generation and generalization in energy-based models},
  author={Du, Yilun and Mordatch, Igor},
  journal={arXiv:1903.08689},
  year={2019}
}


@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}

@article{verdon2019quantum,
  title={Quantum Hamiltonian-Based Models and the Variational Quantum Thermalizer Algorithm},
  author={Verdon, Guillaume and Marks, Jacob and Nanda, Sasha and Leichenauer, Stefan and Hidary, Jack},
  journal={arXiv:1910.02071},
  year={2019}
}


@article{zhai2016deep,
  title={Deep structured energy based models for anomaly detection},
  author={Zhai, Shuangfei and Cheng, Yu and Lu, Weining and Zhang, Zhongfei},
  journal={arXiv:1605.07717},
  year={2016}
}

@inproceedings{swersky2011autoencoders,
  title={On autoencoders and score matching for energy based models},
  author={Swersky, Kevin and Buchman, David and Freitas, Nando D and Marlin, Benjamin M and others},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={1201--1208},
  year={2011}
}

@article{finn2016connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1352--1361},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{dahl2010phone,
  title={Phone recognition with the mean-covariance restricted Boltzmann machine},
  author={Dahl, George and Ranzato, Marc'Aurelio and Mohamed, Abdel-rahman and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={469--477},
  year={2010}
}

@article{larochelle2012learning,
  title={Learning algorithms for the classification restricted boltzmann machine},
  author={Larochelle, Hugo and Mandel, Michael and Pascanu, Razvan and Bengio, Yoshua},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Mar},
  pages={643--669},
  year={2012}
}




@article{van_hemmen_spin-glass_1986,
	title = {Spin-glass models of a neural network},
	volume = {34},
	issn = {0556-2791},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.34.3435},
	doi = {10.1103/PhysRevA.34.3435},
	pages = {3435--3445},
	number = {4},
	journaltitle = {Physical Review A},
	author = {van Hemmen, J. L.},
	urldate = {2019-05-10},
	date = {1986-10-01},
	langid = {english},
	file = {van Hemmen - 1986 - Spin-glass models of a neural network.pdf:/Users/phuembeli/Zotero/storage/STTYYLFA/van Hemmen - 1986 - Spin-glass models of a neural network.pdf:application/pdf}
}


@book{rojas2013neural,
  title={Neural networks: a systematic introduction},
  author={Rojas, Ra{\'u}l},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{lecun2006tutorial,
  title={A tutorial on energy-based learning},
  author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia},
  year={2006}
}

@article{aurell2012inverse,
  title={Inverse Ising inference using all the data},
  author={Aurell, Erik and Ekeberg, Magnus},
  journal={Physical review letters},
  volume={108},
  number={9},
  pages={090201},
  year={2012},
  publisher={APS}
}

@article{biamonte2008nonperturbative,
  title={Nonperturbative k-body to two-body commuting conversion Hamiltonians and embedding problem instances into Ising spins},
  author={Biamonte, JD},
  journal={Physical Review A},
  volume={77},
  number={5},
  pages={052331},
  year={2008},
  publisher={APS}
}

@article{babbush2013resource,
  title={Resource efficient gadgets for compiling adiabatic quantum optimization problems},
  author={Babbush, Ryan and O'Gorman, Bryan and Aspuru-Guzik, Al{\'a}n},
  journal={Annalen der Physik},
  volume={525},
  number={10-11},
  pages={877--888},
  year={2013},
  publisher={Wiley Online Library}
}

@article{hastings1970monte,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}

@incollection{robert1999metropolis,
  title={The Metropolis—Hastings Algorithm},
  author={Robert, Christian P and Casella, George},
  booktitle={Monte Carlo Statistical Methods},
  pages={231--283},
  year={1999},
  publisher={Springer}
}

@misc{nielsen2002quantum,
  title={Quantum computation and quantum information},
  author={Nielsen, Michael A and Chuang, Isaac},
  year={2002},
  publisher={AAPT}
}

@article{huembeli2019automated,
  title={Automated discovery of characteristic features of phase transitions in many-body localization},
  author={Huembeli, Patrick and Dauphin, Alexandre and Wittek, Peter and Gogolin, Christian},
  journal={Physical Review B},
  volume={99},
  number={10},
  pages={104106},
  year={2019},
  publisher={APS}
}

@article{wetzel2017unsupervised,
  title={Unsupervised learning of phase transitions: From principal component analysis to variational autoencoders},
  author={Wetzel, Sebastian J},
  journal={Physical Review E},
  volume={96},
  number={2},
  pages={022140},
  year={2017},
  publisher={APS}
}

@article{melnikov2018active,
  title={Active learning machine learns to create new quantum experiments},
  author={Melnikov, Alexey A and Nautrup, Hendrik Poulsen and Krenn, Mario and Dunjko, Vedran and Tiersch, Markus and Zeilinger, Anton and Briegel, Hans J},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={6},
  pages={1221--1226},
  year={2018},
  publisher={National Acad Sciences}
}

@article{iten2018discovering,
  title={Discovering physical concepts with neural networks},
  author={Iten, Raban and Metger, Tony and Wilming, Henrik and Del Rio, L{\'\i}dia and Renner, Renato},
  journal={arXiv preprint arXiv:1807.10300},
  year={2018}
}







