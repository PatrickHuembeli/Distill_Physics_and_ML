@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/79/8/2554},
	doi = {10.1073/pnas.79.8.2554},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	pages = {2554--2558},
	number = {8},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Hopfield, J. J.},
	urldate = {2019-05-14},
	date = {1982-04-01},
	langid = {english},
	pmid = {6953413},
	file = {Full Text PDF:/Users/phuembeli/Zotero/storage/QZMJXHZQ/Hopfield - 1982 - Neural networks and physical systems with emergent.pdf:application/pdf;Snapshot:/Users/phuembeli/Zotero/storage/PB7FKUYI/2554.html:text/html}
}

@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{salakhutdinov2007restricted,
  title={Restricted Boltzmann machines for collaborative filtering},
  author={Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={791--798},
  year={2007},
  organization={ACM}
}

@article{du2019implicit,
  title={Implicit generation and generalization in energy-based models},
  author={Du, Yilun and Mordatch, Igor},
  journal={arXiv preprint arXiv:1903.08689},
  year={2019}
}

@article{le2010deep,
  title={Deep belief networks are compact universal approximators},
  author={Le Roux, Nicolas and Bengio, Yoshua},
  journal={Neural computation},
  volume={22},
  number={8},
  pages={2192--2207},
  year={2010},
  publisher={MIT Press}
}

@article{hopfield_unlearning_1983,
	title = {"Unlearning" has a stabilizing effect in collective memories},
	volume = {304},
	rights = {1983 Nature Publishing Group},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/304158a0},
	doi = {10.1038/304158a0},
	abstract = {Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an ‘unlearning’ process. We have independently carried out mathematical and computer modelling of learning and ‘unlearning’ in a collective neural network of 30–1,000 neurones. The model network has a content-addressable memory or ‘associative memory’ which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an ‘unlearning’ process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of ‘unlearning’ in rapid eye movement ({REM}) sleep.},
	pages = {158},
	number = {5922},
	journaltitle = {Nature},
	author = {Hopfield, J. J. and Feinstein, D. I. and Palmer, R. G.},
	urldate = {2019-05-14},
	date = {1983-07},
	file = {Snapshot:/Users/phuembeli/Zotero/storage/NEK3XJ7X/304158a0.html:text/html}
}


@article{amit1985spin,
  title={Spin-glass models of neural networks},
  author={Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, Haim},
  journal={Physical Review A},
  volume={32},
  number={2},
  pages={1007},
  year={1985},
  publisher={APS}
}

@article{cipra1987introduction,
  title={An introduction to the Ising model},
  author={Cipra, Barry A},
  journal={The American Mathematical Monthly},
  volume={94},
  number={10},
  pages={937--959},
  year={1987},
  publisher={Taylor \& Francis}
}

@article{gregor2015draw,
  title={DRAW: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1502.04623},
  year={2015},
  url ={https://arxiv.org/pdf/1502.04623.pdf}
}
@article{mercier2011humans,
  title={Why do humans reason? Arguments for an argumentative theory},
  author={Mercier, Hugo and Sperber, Dan},
  journal={Behavioral and brain sciences},
  volume={34},
  number={02},
  pages={57--74},
  year={2011},
  publisher={Cambridge Univ Press},
  doi={10.1017/S0140525X10000968}
}

@article{dong2014image,
  title={Image super-resolution using deep convolutional networks},
  author={Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  journal={arXiv preprint arXiv:1501.00092},
  year={2014},
  url={https://arxiv.org/pdf/1501.00092.pdf}
}

@article{carreira-perpinan_contrastive_nodate,
	title = {On Contrastive Divergence Learning},
	abstract = {Maximum-likelihood ({ML}) learning of Markov random Ô¨Åelds is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a diÔ¨Äerent function called ‚Äúcontrastive divergence‚Äù ({CD}). {CD} learning has been successfully applied to various types of random Ô¨Åelds. Here, we study the properties of {CD} learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast {CD} learning can therefore be used to get close to an {ML} solution and slow {ML} learning can then be used to Ô¨Åne-tune the {CD} solution.},
	pages = {8},
	author = {Carreira-Perpinan, Miguel A and Hinton, GeoffÄrey E},
	langid = {english},
	file = {Carreira-Perpinan and Hinton - On Contrastive Divergence Learning.pdf:/Users/phuembeli/Zotero/storage/P46N5NNY/Carreira-Perpinan and Hinton - On Contrastive Divergence Learning.pdf:application/pdf}
}

@article{dumoulin2016adversarially,
  title={Adversarially Learned Inference},
  author={Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Lamb, Alex and Arjovsky, Martin and Mastropietro, Olivier and Courville, Aaron},
  journal={arXiv preprint arXiv:1606.00704},
  year={2016},
  url={https://arxiv.org/pdf/1606.00704.pdf}
}

@article{dumoulin2016guide,
  title={A guide to convolution arithmetic for deep learning},
  author={Dumoulin, Vincent and Visin, Francesco},
  journal={arXiv preprint arXiv:1603.07285},
  year={2016},
  url={https://arxiv.org/pdf/1603.07285.pdf}
}

@article{gauthier2014conditional,
  title={Conditional generative adversarial nets for convolutional face generation},
  author={Gauthier, Jon},
  journal={Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester},
  volume={2014},
  year={2014},
  url={http://www.foldl.me/uploads/papers/tr-cgans.pdf}
}

@article{johnson2016perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1603.08155},
  year={2016},
  url={https://arxiv.org/pdf/1603.08155.pdf}
}

@article{mordvintsev2015inceptionism,
  title={Inceptionism: Going deeper into neural networks},
  author={Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
  journal={Google Research Blog},
  year={2015},
  url={https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}

@misc{mordvintsev2016deepdreaming,
  title={DeepDreaming with TensorFlow},
  author={Mordvintsev, Alexander},
  year={2016},
  url={https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb},
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015},
  url={https://arxiv.org/pdf/1511.06434.pdf}
}

@inproceedings{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2226--2234},
  year={2016},
  url={https://arxiv.org/pdf/1606.03498.pdf}
}

@article{shi2016deconvolution,
  title={Is the deconvolution layer the same as a convolutional layer?},
  author={Shi, Wenzhe and Caballero, Jose and Theis, Lucas and Huszar, Ferenc and Aitken, Andrew and Ledig, Christian and Wang, Zehan},
  journal={arXiv preprint arXiv:1609.07009},
  year={2016},
  url={https://arxiv.org/pdf/1609.07009.pdf}
}

@misc{openai2018charter,
  author={OpenAI},
  title={OpenAI Charter},
  type={Blog},
  number={April 9},
  year={2018},
  url={https://blog.openai.com/charter},
}
