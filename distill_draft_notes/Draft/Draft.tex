\documentclass[nofootinbib, superscriptaddress, prl]{revtex4}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{braket}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}
\title{Machine Learning, Physics and Equilibrium}
\author{Patrick, Juan-Miguel, Arthur, Peter, Nathan}
\maketitle

\section{Outline}

\begin{itemize}
\item Split whole text into testing and training!
\item MaxEntropy leads to BM distribution
\item Thermal equilibrium (without big explanations): The BM distr describes thermal equilibrium of stat mech systems for example spin systems.
We can factor T=1 or we can also factor the temperature into what we call weights and biases in ML.
\item Sufficient statistics --> Ising
\item Energy (energy based models) and Temp, mention Ising
\item Sampling in a Hopfield model (or generally from a computational point of view) Finds closest local minimum. Zero temp BM.
\item Sampling in physics (equilibrate from initial random state)
\item visible, hidden partitioning from computer science view, is to increase expressivity.
(not long range correlations, this argument comes from RBM). Avoid higher order interactions. By introducing hidden units.
\item physics of hidden units? it is physical impossible to have states of that high degeneracy?
\item Restricted BM from CS perspective brings efficiency. It is possible to sample
\item In physics, talk about long range correlations. (This probably only makes sense in Restricted BM)
Find system where we only look at parts.
\item quasi static evolution as a training approach.
\item I would like to include the idea of what do we need for a ML algorithm. We need to be able to sample, to compare somehow the samples and the data and finally 
\item we will need to talk about higher-order correlations. That they are important in ML and that we can implement them with the visible units. Here we also should connect ML and physics.
\item mention somewhere in beginning that this not yet BM. It is just Ising model.
\end{itemize}

\section{New Try}

\subsection{Introduction}

Boltzmann machines have an interesting history. Boltzmann was an Austrian scientist who worked on statistical descriptions of physics in the 1800s. His name is tied to a number of theories and results that are still in common use today: Boltzmann's constant, Boltzmann's equation, Maxwell-Boltzmann statistics, and the Boltzmann distribution. Boltzmann did not, however, invent Boltzmann machines. This is a more recent construct, emerging from the machine learning literature of the 1980s. Essentially, machine learning researchers "ported'' certain ideas which were originally developed for statistical physics. In particular, the idea that the probability for a system to be in a particular configuration $x$ can be specified using a single scalar function -- the \emph{energy} $E(x)$ -- of that configuration. 

Boltzmann machines were further developed, extended, and improved, over several decades. Currently, the hottest topic in machine learning is deep neural networks, and Boltzmann machines -- a kind of shallow neural network -- have fallen somewhat out of favour amongst practitioners. Despite this, Boltzmann machines have recently been ``back-ported'' to physics, where they have successfully been used to model the wavefunctions of quantum systems \cite{carleo2017solving}. And they also seem to get back into the focus of fundamental research in machine learning 

\noindent\fbox{%
    \parbox{\textwidth}{%
       Yoshua Bengio tweeted about this: \url{https://twitter.com/datasciencenig/status/1020355546581553152}, I don't find the original tweet at the moment. But might be appropriate in a blog to "cite" one of the leading ML researchers that says we should go back to study RBM to understand deep learning.
    }%
}

Furthermore Boltzmann machines are still being used in other areas, such as ... (need to see if anyone is using them).

To understand the "Physics of Boltzmann Machines" we structure this article the following way:
Our article is split into two main section. In the first section we introduce the framework of a Boltzmann machine and describe the functionality of this model independent from the context of learning and training the model. 
In the second section on the other hand we will describe the training procedure in detail. 
We will start with the principle of maximum entropy which leads to the Boltzmann distribution. The Boltzmann distribution itself describes a system at thermal equilibrium at finite temperature. The nature of this system will be defined by the choice of the constraints under which we want to maximise the entropy and the sufficiency of their statistics. We will have a brief discussion about energy based models and the connection of energy and probability which will lead us to the first concrete ML model, the Hopfield network. We will draw the connection of the sampling procedure of these models and how physical system equilibrate. The introduction of hidden and visible units will finally lead us to the Boltzmann machine. We will emphasise why the use of hidden units is beneficial for ML purposes but also what it physically means. And finally we will emphasise the importance of the introduction of a bipartition of the BM graph which is commonly known as a restricted BM.

In the second section we give an overview of how RBMs can be trained and introduce the contrastive divergence algorithm. We also provide a physical interpretation of this algorithm to justify from a physical point of view why this procedure makes sense.

\subsection{Testing: The BM as a trained model}

\subsubsection{The principle of maximum entropy}

\emph{Jaynes' principle} is named after the 19th century physicist E. T. Jaynes. This principle, also called the \emph{principle of maximum entropy}, tells us that, amongst all distributions which are consistent with known observations (e.g., the expectation values of certain random variables), the distribution with maximal entropy should be preferred. Known observations in machine learning are for example the statistics of single input pixels or the correlations between pixels of the data.

Intuitively, it makes sense to choose the distribution with maximum entropy. If we do not have any information and therefore no constraint about a particular degree of freedom of a system, we should remain maximally flexible in our choice of model, while remaining consistent with the degrees of freedom about which we do have strong beliefs and that are constraint. Choosing the maximum entropy model reduces the amount of (potentially biased and unsubstantiated) prior information built into a model. Jaynes showed how to arrive at the above Boltzmann equation from the principle of maximum entropy, and derivations can be found in several textbooks. \textbf{Could linke Susskind's Stanford video where he derives it. Very well done. \url{https://www.youtube.com/watch?v=SmmGDn8OnTA} around 45:00 minutes.}

Suppose we have a system which can exist in many different possible configurations $\{\vec{x}_j\}_{j=1}^J$. We assume a discrete set of configurations, but similar arguments can be made for a continuous set. We have some partial information about the system, specifically the expectation values of the random variables $\{r_k(\vec{x})\}_{k=1}^K$ (these can be constraints forced upon the system by us, or actually observed via measurement). That is, we have the constraints 
\begin{equation}
 \langle r_k \rangle_{x} = a_k,
\end{equation}

with $ \langle r_k \rangle_{x} = \sum_i p(\vec{x}_i) r_k(\vec{x}_i)$.
These expectation values are not enough to completely characterize the system (i.e. if we have more configurations than constraints, $J>K$), and we have no information about random variables outside of the span of the $r_k$. Jaynes' principle tells us that, if we want to model this system, we should maximize the entropy $H(x) = -\sum_{j=1}^J p(\vec{x}_j) \log p(\vec{x}_j)$, subject to the given constraints, plus we need to take care that $p(\vec{x})$ is normalized $\sum_{j=1}^J p(\vec{x}_j) = 1 $. Thus, we try to solve the constrained optimization
\begin{equation}
 \textit{max}_{\lambda_i}[-\sum_{j=1}^J p(\vec{x}_j) \log p(\vec{x}_j) + \sum_{k=1}^K \lambda_k (\langle r_k \rangle_{\vec{x}_j} - a_k) + \lambda_0(\sum_{j=1}^J p(\vec{x}_j) - 1)],
\end{equation}

which has the solution

\begin{equation}
p(\vec{x}_j) = \frac{1}{Z} \exp \left( \sum_k^K \lambda_k  r_k (\vec{x}_j)\right)
\end{equation}

This probability distribution is known under the name Boltzmann distribution.

\subsubsection{Boltzmann distribution and Equilibrium}

The Boltzmann distribution is known from physics and it describes the statistics of physical configurations of a system that is in an equilibrium at some temperature. In this case the probability of a configuration $\vec{x}$ is given as $p(\vec{x}) = \frac{1}{Z} \exp{- \frac{1}{T} E(\vec{x})}$. Where $E(\vec{x})$ is a function that connects the configuration $\vec{x}$ with the real line, which is also called the energy of the system. Before, we did not explicitely write the temperature because we either set it to $T=1$ or it is already absorbed in the parameters $\lambda_k$ of the model. But it is important that we are looking at systems with non-zero temperature because for zero temperature a physical system will equilibrate to its state of minimum energy. Allowing a non zero temperature automatically leads to states that are a thermal distribution which means we would observe many possible configurations that are not only of minimum energy, but at low energy. In the extreme case of very high temperature all the configurations even get equally likely.  

\textbf{Add here modified version of 2-level system figure. Maybe we have to show an energy landscape and the occupation probability of two states. We should discuss this}

\subsubsection{From the sufficient statistics to the Energy function}

So far this result is general and can be applied to any constraint, the choice of $\langle r_k \rangle_x$ will in the end define the physics of the model. The most commonly used physical model in machine learning that defines the underlying behaviour of the Boltzmann distribution is a so called Ising model, which is defined through a energy function $E(\vec {\sigma}) = \sum_{i,j} w_{i,j} \sigma_i \sigma_j + \sum_i h_i \sigma_i$. 
The sufficient statistic of the Boltzmann distribution of an Ising model are the single-spin expectation values $\langle \sigma_i \rangle$ and the two-spin correlations $\langle \sigma_i \sigma_j \rangle$. This means that these two constraints suffice to fully determine the parameters of the classical Ising energy function from the data.
Generally for the exponential family, a statistic $T(x)$ is sufficient, if we can write the probability $p(x)$ as

\begin{align}
	p(x) = \exp \left( \alpha(\theta)^T T(x) + A(\theta) \right),
\end{align}

where $\alpha(\theta)$ is a vector valued function and $A(\theta)$ is a scalar which for a Boltzmann distribution is simply a normalization factor that we call the partition function $A(\theta) = \log(1/Z)$ \cite{li_learning_2013}. Therefore in general we are not restricted to model the distribution of our data with the energy function of a classical Ising model. The choice of the constraints will determine the physical model.

We want to emphasise at this point that the choice of single-spin and two-spin expectation values for this kind of model is not sufficient to capture higher-order correlations between input nodes. For real-world ML applications these correlations are important and this kind of model does not suffice. But we also want to stress here that we are not talking about BM yet. So far this model is simply an Ising spin system, where every spin represents a node (e.g. pixel) of our input data. To overcome this issue of low order correlations later we will use a part of the spins in the system as mediators between input nodes, which we we call hidden units.

\subsubsection{Paragraph for Boltzman distribution at zero temperature??}

\subsubsection{Energy-based models}

The energy function is not unique to Boltzmann machines and occurs in a huge class of models that are summarized under the name energy-based models. The basic idea of energy-based models is to capture the different possible configurations of a system through a single scalar quantity, called the energy. Low-energy configurations are more likely than high-energy configurations, and if two configurations have the same energy, they are equally likely. The Boltzmann distribution connects energy to probability via the formula
\begin{equation}
 p(x) \propto \exp(-\beta E(x)),
\end{equation}
where $\beta$ is an arbitrary positive number (in physics, this is called the \emph{inverse temperature}). 
There are a number of arguments which can lead us to this formula. The simplest is perhaps that the exponential function is the most straightforward way to map the real line (the domain of $E(x)$) into positive numbers, which we could then normalize to probabilities. A more elaborate way to define it is the principle of maximum entropy that we have shown before.

\subsubsection{Architecture}

\textbf{We can make this section also as a figure. With 3 collumns, showing the models and the text below. But I think it is a good idea }

So far we have not restricted ourselves to a certain kind of model. But for practical purposes some models have been proven to be more useful than others and the most common model is based on the energy function of the Ising spin model. The first model we introduce is the Hopfield network, which is simply an Ising model of $N$ spins at zero temperature, where the dimension of the data vector is equal to the number of available spins. This kind of model does not really learn configurations, it just memorizes them. 

The Boltzmann machine is in two aspect different from the Hopfield network. First it is not at zero temperature anymore, therefore we allow thermal distributions of configurations and not only local minima of the energy. And second the $N$ spins of the model are separate into $v$ visible units and $h$ hidden units. The dimension of the input data is equal to the visible units $v$.

And finally the Restricted Boltzmann machine is the model that is most commonly used. In terms of learning and expressivity of this model there is no advantage of the restricted compared to the fully-connected BM. We will see later in the training section that there is a mayor advantage of the RBM over the BM in terms of computability of the gradients.

\subsection{Sampling}

\textbf{We should add the plot here with the nodes that equilibrate, to show that equilibration makes random inputs equilibrate. The problem here is, that we have not yet introduced the BM. We are still with general spin models.}

\subsubsection{Sampling in physics}
We would like to think of energy based models the following way. Imagine you are an experimentor and you have an implementation of a spin system in front of you. You can initialize the spins and you can manipulate the couplings $J_{i,j}$ and the local fields $h_i$. The only thing you can do with such a machine is to initialize the spins in some way and see to what configuration they will equilibrate.
I physics low energies are preferred over high energies. 
And therefore if we initialize a system randomly it will eventually equilibrate to a low energy configuration or if we are at $T=0$ to a local minimum of the energy.
Therefore in a real physical system, sampling from a system is only a matter of initialising it and waiting until it is equilibrate.

\subsubsection{Sampling in ML}

In ML we don't have an equilibration process that drives our system into a low energy configuration. But there are several methods how we can imitate this behaviour.
To sample from a Hopfield network one takes a random input vector $\vec{v}$ and updates each node after another and then determines the update of a node via the update rule $x_i = 1,~ if~\sum_j W_{ij} x_j \geq h_i$. The outcome in this case is deterministic and one can stop this process after the nodes have been iterated through two times without any update.
For non-zero temperature models, the updates work similarly, just that the update is not deterministic, but there is a probability assigned to every node $x_i$ according to its input strength $\sum_j W_{ij} x_j + h_i$. And according to this probability we update the spins.

\subsection{Training}

If we go back to image of a experimenter standing in front of a physical implementation of a energy function the question now arises, what do we need, to train such a system on a given data set. Because as discussed before the only thing we can do is sampling from this machine. Therefore training a BM means that we have to come up with an update rule for the model parameters simply by comparing the samples with the data. In this section we will have a deeper look at this procedure. 

\subsection{Learning: From Ising models to Boltzmann machines to restricted Boltzmann machines}

After one has agreed on a certain kind of model the learning or training procedure itself can be interpreted as the search for the parameters of the system Hamiltonian which maximize the entropy and fulfil the constraints. If we consider a Ising model the constraints are given by the expectation values $\langle \sigma_i \sigma_j \rangle $
 and $\langle \sigma_i \rangle$ and the coupling parameters $w_{i,j}$ of the Hamiltonian control the expectation values of $\langle \sigma_i \sigma_j \rangle $ and the local fields $h_i$ controll the single spin expectations $\langle \sigma_i \rangle$.
To learn these system parameters we slowly change the parameters of the system until the expectation values of the model get closer of the expectation values of the data. 

\subsubsection{Maximum likelihood estimation (MLE) DO WE KEEP THIS?}

We would like to add a few words here to close the loop between the principle of maximum entropy and the maximum likelihood estimation, especially for people who come from machine learning.
Energy based models and predominantly Boltzmann machines are approached differently in machine learning literature. BMs are a choice for an ansatz for the probability distribution and the parameters of the model are adjusted such that the data is maximally likely. Therefore the expectation values $\langle \sigma_i \rangle$ and  $\langle \sigma_i \sigma_j \rangle$ are not constraints of the model they are the objectives to be maximally likely.
The choice of the Boltzmann machine as an ansatz for the probability distribution in machine learning is very well motivated by its success in experiments (Cite some RBM papers), but for a physicist intuitively the question comes to mind, why should we not use any other energy function that we know from physics. When we approach energy based models from the principle of maximum entropy we can see that the constraints and therefore the data itself determines the model and therefore from a theoretical point of view we are not restricted to Ising like energy functions.

\subsubsection{The Hopfield network}

At the very beginning of this section we need to go one step back and briefly discuss the Hopfield network \cite{Hopfield1982NeuralNA}. The Hopfield network is a so called associative memory network, which does not learn patterns and generalise them to new ones, it only memories the training data and can recreate it after memorising them if one only feeds part of the data in the network. The Hopfield network has exact the same energy function $E(\vec {\sigma}) = -\sum_{i,j} w_{i,j} \sigma_i \sigma_j + \sum_i h_i \sigma_i$, but the nodes are not activated probabilistically, they follow the deterministic update rule

\begin{equation}
\label{Eq:Hopfield update}
	\sigma_{i}\leftarrow \left\{{\begin{array}{ll}+1~{\mbox{if }}\sum _{{j}}{w_{{ij}}\sigma_{j}}\geq h _{i},\\-1~{\mbox{otherwise,}}\end{array}}\right.
\end{equation}

which can be interpreted as a Ising model at zero temperature, because this way a random input configuration $\vec{\sigma}$ will always converge to the next local minimum. No fluctuations because of temperature will occur. 
The "training" of a Hopfield network is also deterministic and we update the weight matrix simply with the outer product of the $n$ input vectors $\{ \vec{\sigma}^{\alpha}\}_{\alpha}^n$ that have to be memorised. $W = \sum_{\alpha}^n( (\vec{\sigma}^{\alpha})^T \vec{\sigma}^{\alpha} - \mathbb{1}) $, which can be written as an update rule $w_{i,j} \leftarrow \sum_{\alpha}^n \sigma_i^{\alpha} \sigma_j^{\alpha}, \forall~i \neq j$
%= \sum_i^n \vec{\sigma}_i^T \vec{\sigma}_i - n \mathbb{1}$
, the subtraction of the identity matrix takes care that the nodes are not connected with themselves. The average over all the data instances $\alpha$ is also often written as $\langle \sigma_i \sigma_j \rangle_{\text{Data}}$.
This weight matrix leads to a energy landscape where each training vector is exactly a local minimum. And if we feed a slight variation of one of the input vectors into the network, the update rule will make them converge to the configuration that is associated with the closest energy minimum. 
The memory capacity of Hopfield networks is very limited and the more data we want to memorize, the higher is the chance to get so called spurious minima. These are local energy minima which minimize the energy for configurations that are not part of the training and therefore will lead to wrong memories. 
%After memorising the input vectors an arbitrary vector $\vec{\sigma}$ can be fed through the network and $W \vec{\sigma} =  \vec{\sigma}  \vec{\sigma}_1^T \vec{\sigma}_1 + \vec{\sigma}  \vec{\sigma}_2^T \vec{\sigma}_2 + \dots -  \vec{\sigma} n \mathbb{1} = $

Source: \url{http://page.mi.fu-berlin.de/rojas/neural/chapter/K13.pdf}

It is worth mentioning here that in \cite{hopfield1983unlearning} Hopfield studied the effect of "unlearning" on the performance of Hopfield networks. They found that it can help to get rid of supurous minima and therefore has a stabilizing effect on the memory of such a network. 
For unlearning we expand the update rule $w_{i,j} \leftarrow \langle \sigma_i \sigma_j \rangle_{\text{Data}}$ to  $w_{i,j} \leftarrow \langle \sigma_i \sigma_j \rangle_{\text{Data}} - \epsilon \langle \sigma_i' \sigma_j' \rangle$. To obtain $\vec{\sigma}'$, the network is initialized in a random configuration $\vec{\sigma}$ and the nodes are updated according to the update rule Eq. (\ref{Eq:Hopfield update}) until the network equilibrates to $\vec{\sigma}'$. Therefore the configuration $\vec{\sigma}'$ is a minimum of the energy landscape and the term $- \epsilon \langle \sigma_i' \sigma_j' \rangle$ increases the energy of this minimum by a small factor $\epsilon<1$. This plays against the first term of the update rule $\langle \sigma_i \sigma_j \rangle_{\text{Data}}$ which minimizes the energies of the data inputs. But in the end the interplay of these two terms decreses the occurence of spurous minima.


\subsubsection{The Ising model in finite temperature}

Now if we turn on the temperature our model is not fully deterministic anymore and we allow energy states to be occupied that are not the exact energies of local minima.
\textbf{In Figure such and such } one can try to adjust the parameters of the system in a way that the model distribution fits the target distribution (in a certain background color). One will see that this task is not easy because all the parameters influence each other and therefore one has to make small adjustments with each step.

If we now go to a bit more difficult target distribution one will realize that it cannot be approximated with the model at hand. So far we only used a Ising system to model data, where every spin represents a data point. A much more expressive model is the Boltzmann machine, where we start to distinguish between visible and hidden spin variables. On first glance there is no difference between the two models except the name of the nodes. But the crucial point is that only the visible units are used to represent data. The hidden units are only there to increase the space of possible configurations and they will be averaged out in the end. This way we can increase the expressivity of the model. \textbf{hidden nodes as mediator between visible units for higher order interactions} In Figure (such and such) one can try again to model the harder distribution but now with hidden nodes. You will see that it is now possible to approximate more complex target distributions.

\subsubsection{R! BM and sampling}

 \textbf{Need to introduce RESTRICTED! BM, why do we do that? --> sampling}

So far we assumed that we have access to the probability distribution of the energy based model $p(x, \theta)$ and the data $p(x)$. But in general this is not true because the partition function $Z$ is not tractable for systems of the size $N>20$. Therefore to compare the statistics of the data with the statistics of the model we need to be able to approximate these distributions. For the data this is relatively simple, because we can just average over some data instances, for example over batches. To approximate the model distribution we need configurations that come from the model itself. For a real physical system we therefore would set the parameters of the model and wait until it equilibrates, take many configurations like this and compare them to the data. 
Unfortunately we cannot simulate these equilibration dynamics on a computer, because they are computationally very demanding. What we can do is Gibbs sampling. But for a BM Gibbs sampling is as well not tractable, because if we sample one variable a change would influence the probabilities of all other variables and equilibration would take very long time. \textbf{I am not 100 \% sure if it even would equilibrate} To avoid this problem the graph of the BM can be made bipartite, which means that we separate the nodes in a visible and a hidden layer and don't allow inter-layer connections. This architecture is called "Restricted Boltzmann Machine". With this simple trick the conditional probabilities $p(\vec{v}|\vec{h})$ and $p(\vec{h}|\vec{v})$ marginalize and therefore we can sample each node of a layer independent of the other nodes of the same layer. 

The question that now remains is how can we efficiently adjust the parameters? Can we somehow derive an update rule simply by comparing the samples from the model with the data? The answer is yes and it is known under the name Contrastive Divergence. If we use the update rule $W \rightarrow W - \Delta W$ for gradient descent the update rule of CD is:

\begin{equation*}
	\Delta W_{i,j} = \langle \frac{ \partial E}{\partial W_{i,j}} \rangle_{\text{Data}} - \langle \frac{\partial E}{\partial W_{i,j}} \rangle_{p_{\theta}} %= \langle v_i h_j \rangle_{\text{Data}} - \langle v_i h_j \rangle_{p_{\theta}}
\end{equation*}

If the energy is given by $E(v,h) = \sum_{i,j} W_{i,j} v_i h_j $ (\textbf{sometimes there is a minus sign in front of the sum, which changes the direction}).

But what is the intuition about this update rule, why is the minus sign in front of the average over the model distribution and not vice versa. How do we know in which direction we will have to move my parameters, if we train a model.
To get some intuition about this update rule we consider the very extreme case of a system whit only two spins $v$ and $h$ and assume that these variables are always not equal in the data and always equal if we sample them from the distribution $p_{\theta}$. This means that $\langle v h \rangle_{\text{Data}} = -1$
 and $\langle v h \rangle_{p_{\theta}}  = 1$. So the update will be minimally, namely $\Delta W_{i,j} = -2$ and therefore we move to higher weights $W$. The data is always anti-aligned, which means the minimum energy of the configurations of the data will be achieved by increasing the weights to be strongly positive. And since the configurations sampled from the distribution $p_{\theta}$ are always aligned $W$ must be strongly negative at the moment because this has to be minimum energy.
 

 
\section{Connecting training with equilibrium}

Machine learning with energy-based models bears some strong resemblance to long-studied subfields of physics. For instance, in machine learning we want to model systems which could have (exponentially) many possible configurations using only a small set of parameters. A similar approach is used in the study of statistical physics and thermodynamics, where we describe thermodynamic systems by an energy function (sometimes referred to as a potential) which consists of a very small set of parameters (temperature, pressure, etc.). If two systems, with different values of these parameters, are allowed to interact, energy will flow -- in both directions -- between them . This energy exchange process causes the system's thermodynamic parameters to change values. As we all know, putting food in the oven -- whose temperature is typically very hot -- will cause the food to heat up. Leaving hot food out in the kitchen will cause it to cool down again. 




 
\begin{thebibliography}{10}

\bibitem{carleo2017solving}
G.~Carleo, M.~Troyer, {\it Science\/} {\bf 355, 6325}, 602
  (2017).

\bibitem{li_learning_2013}
X.~Li, W.~Bin, L.~Yuncai, L.~Tai~Sing, {\it Machine Learning and Knowledge Discovery in Databases}, {\bf 49--64} (2013)

\bibitem{Hopfield1982NeuralNA}
J.~J.~Hopfield, {\it Proceedings of the National Academy of Sciences of the United States of America}, {\bf 79 (8)}, 2554-8 (1982) 

\bibitem{hopfield1983unlearning}
J.~J.~Hopfield, D.~I.~Feinstein, R.~G.~Palmer, {\it Nature}, {304}, 158, (1983)


\end{thebibliography} 

%\section{Introduction}
%
%\noindent\fbox{%
%    \parbox{\textwidth}{%
%        If the title will be "the physics of Boltzmann machines" it will make sense to start with a short introduction into BM and after that start more general with energy based models. But this is still open for discussion.
%    }%
%}
%
%Boltzmann machines have an interesting history. Boltzmann was an Austrian scientist who worked on statistical descriptions of physics in the 1800s. His name is tied to a number of theories and results that are still in common use today: Boltzmann's constant, Boltzmann's equation, Maxwell-Boltzmann statistics, and the Boltzmann distribution. Boltzmann did not, however, invent Boltzmann machines. This is a more recent construct, emerging from the machine learning literature of the 1980s. Essentially, machine learning researchers ``ported'' certain ideas which were originally developed for statistical physics. In particular, the idea that the probability for a system to be in a particular configuration $x$ can be specified using a single scalar function -- the \emph{energy} $E(x)$ -- of that configuration. 
%
%Boltzmann machines were further developed, extended, and improved, over several decades. Currently, the hottest topic in machine learning is deep neural networks, and Boltzmann machines -- a kind of shallow neural network -- have fallen somewhat out of favour amongst practitioners. Despite this, Boltzmann machines have recently been ``back-ported'' to physics, where they have successfully been used to model the wavefunctions of quantum systems \cite{carleo2017solving}. And they also seem to get back into the focus of fundamental research in machine learning 
%
%\noindent\fbox{%
%    \parbox{\textwidth}{%
%       Yoshua Bengio tweeted about this: \url{https://twitter.com/datasciencenig/status/1020355546581553152}, I don't find the original tweet at the moment. But might be appropriate in a blog to "cite" one of the leading ML researchers that says we should go back to study RBM to understand deep learning.
%    }%
%}
%
%Furthermore Boltzmann machines are still being used in other areas, such as ... (need to see if anyone is using them).
%
%To understand the "Physics of Boltzmann Machines" we will start with more general models, the energy-based models, that will eventually lead us to the concept of Boltzmann machines and finally to restricted Boltzmann machines which are the most commonly used models in machine learning.



%\section{Energy-based models and the Boltzmann distribution}
%
%The basic idea of energy-based models is to capture the different possible configurations of a system through a single scalar quantity, called the energy. Low-energy configurations are more likely than high-energy configurations, and if two configurations have the same energy, they are equally likely. The Boltzmann distribution connects energy to probability via the formula
%\begin{equation}
% p(x) \propto \exp(-\beta E(x)),
%\end{equation}
%where $\beta$ is an arbitrary positive number (in physics, this is called the \emph{inverse temperature}). 
%There are a number of arguments which can lead us to this formula. The simplest is perhaps that the exponential function is the most straightforward way to map the real line (the domain of $E(x)$) into positive numbers, which we could then normalize to probabilities. 

%A stronger justification comes from \emph{Jaynes' principle}, after the 19th century physicist E. T. Jaynes. This principle, also called the \emph{principle of maximum entropy}, tells us that, amongst all distributions which are consistent with known observations (e.g., the expectation values of certain random variables), the distribution with maximal entropy should be preferred. Known observations in machine learning are for example the statistics of single input pixels or the correlations between pixels of the data.
%
%Intuitively, it makes sense to choose the distribution with maximum entropy. If we do not have any information and therefore no constraint about a particular degree of freedom of a system, we should remain maximally flexible in our choice of model, while remaining consistent with the degrees of freedom about which we do have strong beliefs and that are constraint. Choosing the maximum entropy model reduces the amount of (potentially biased and unsubstantiated) prior information built into a model. Jaynes showed how to arrive at the above Boltzmann equation from the principle of maximum entropy, and derivations can be found in several textbooks. \textbf{Worth going through derivation? \textcolor{red}{I don't think this will be necessary} It's just constrained optimization, i.e., maximizing the entropy formula with Lagrange multipliers.}
%
%Suppose we have a system which can exist in many different possible configurations $\{\vec{x}_j\}_{j=1}^J$. We assume a discrete set of configurations, but similar arguments can be made for a continuous set \textbf{I don't think we have to mention this here. I would rather keep it simple.}. We have some partial information about the system, specifically the expectation values of the random variables $\{r_k(\vec{x})\}_{k=1}^K$ (these can be constraints forced upon the system by us, or actually observed obtained via measurement). That is, we have the constraints 
%\begin{equation}
% \langle r_k \rangle_{x} = a_k,
%\end{equation}
%
%with $ \langle r_k \rangle_{x} = \sum_i p(\vec{x}_i) r_k(\vec{x}_i)$.
%These expectation values are not enough to completely characterize the system (i.e. if we have more configurations than constraints, $J>K$), and we have no information about random variables outside of the span of the $r_k$. Jaynes' principle tells us that, if we want to model this system, we should maximize the entropy $H(x) = -\sum_{j=1}^J p(\vec{x}_j) \log p(\vec{x}_j)$, subject to the given constraints, plus we need to take care that $p(\vec{x})$ is normalized $\sum_{j=1}^J p(\vec{x}_j) = 1 $. Thus, we try to solve the constrained optimization
%\begin{equation}
% \textit{max}_{\lambda_i}[-\sum_{j=1}^J p(\vec{x}_j) \log p(\vec{x}_j) + \sum_{k=1}^K \lambda_k (\langle r_k \rangle_{\vec{x}_j} - a_k) + \lambda_0(\sum_{j=1}^J p(\vec{x}_j) - 1)],
%\end{equation}
%
%which has the solution
%
%\begin{equation}
%p(x) = \frac{1}{Z} \sum_j^J \exp \left( \sum_k^K \lambda_k  \langle r_k \rangle_{x_j}\right)
%\end{equation}
%
%\textcolor{red}{Notation is not very clear yet. We probably should change it to something like: $\sum_i p_i r_{ij} = a_j$ and then follows }




%\section{Boltzmann distribution and Equilibrium}
%
%The Boltzmann distribution is known from physics.
%and it describes the statistics of physical configurations of a system that is in an equilibrium at some temperature. We do not explicitely write the temperature because we either set it to $T=1$ or it is already absorbed in the parameters $\lambda_k$ of the model. But it is important that we are not looking at systems with zero temperature because in this case a physical system always will equilibrate to its state of minimum energy. Allowing a non zero temperature automatically leads to states that are a thermal distribution which means we would observe many possible configurations that are not only of minimum energy. In the extreme case of very high temperature all the configurations even get equally likely.  

%\section{From the sufficient statistics to the Hamiltonian}
%
%So far this result is general and can be applied to any physical system. The choice of the constraints $\langle r_k \rangle_x$ will in the end define the physics of the enrgy based model. The most commonly used energy function in machine learning that defines the underlying physics of the Boltzmann distribution is a so called Ising model with $E(\vec {\sigma} = \sum_{i,j} w_{i,j} \sigma_i \sigma_j + \sum_i h_i \sigma_i$. 
%The sufficient statistic of the Boltzmann distribution of an Ising model are the single-spin expectation values $\langle \sigma_i \rangle$ and the two-spin correlations $\langle \sigma_i \sigma_j \rangle$. This means that these two constraints suffice to fully determine the parameters of a classical Ising Hamiltonian from the data.
%Generally for the exponential family a statistic $T(x)$ is sufficient, if we can write the probability $p(x)$ as
%
%\begin{align}
%	p(x) = \exp \left( \alpha(\theta)^T T(x) + A(\theta) \right),
%\end{align}
%
%where $\alpha(\theta)$ is a vector valued function and $A(\theta)$ is a scalar which for a Boltzmann distribution is simply $A(\theta) = \log(1/Z)$. Therefore in general we are not restricted to model the distribution of our data with the energy function of a classical Ising model. The choice of the constraints will determine the physical model.
%
%\textbf{Source:}
%\url{https://link-springer-com.recursos.biblioteca.upc.edu/content/pdf/10.1007%2F978-3-642-40994-3.pdf}, Learning Discriminative Sufficient Statistics
%Score Space for Classification
%
%\noindent\fbox{%
%    \parbox{\textwidth}{%
%       One thing which is not yet fully clear to me is, how do we justify the choice of an Ising model. Is there a good argument why 2-spin correlators are sufficient to learn classical data? Why should we not go to higher momenta? Or would the model even get better if we went to higher momenta? 
%       
%       I think we should add here a few words, why it is reasonable to start with spin-1/2 systems and take an Ising Hamiltonian, just to make the story telling more coherent.
%       
%       And additionally we could also say that if we want to use a Ising like system the constraints are automatically given by the single- and two-spin expectations.
%    }%
%}

%\subsection{Maximum likelihood estimation (MLE)}
%
%We would like to add a few words here to close the loop between the principle of maximum entropy and the maximum likelihood estimation, especially for people who come from machine learning.
%Energy based models and predominantly Boltzmann machines are approached differently in machine learning literature. BMs are a choice for an ansatz for the probability distribution and the parameters of the model are adjusted such that the data is maximally likely. Therefore the expectation values $\langle \sigma_i \rangle$ and  $\langle \sigma_i \sigma_j \rangle$ are not constraints of the model they are the objectives to be maximally likely. \textbf{This sentence needs review!}
%The choice of the Boltzmann machine as an ansatz for the probability distribution in machine learning is very well motivated by its success in experiments (Cite some RBM papers), but for a physicist intuitively the question comes to mind, why should we not use any other energy function that we know from physics. When we approach energy based models from the principle of maximum entropy we can see that the constraints and therefore the data itself determines the model and therefore from a theoretical point of view we are not restricted to Ising like energy functions.
%
%\section{Sampling in physics}
%
%I physics low energies are preferred over high energies. 
%And therefore if we initialize a system randomly it will eventually equilibrate to a low energy configuration or if we are at $T=0$ to the minimum energy.
%If we imagine to have a spin model implemented as a physical device and we can initialise the spins in a random way, this spin configuration would automatically go to a configuration of lower energy.
%Therefore in a real physical system, sampling from a system is only a matter of initialising it and waiting until it is equilibrate.
%
%\section{Sampling in ML}
%
%In ML we don't have an equilibration process that drives our system into a low energy configuration. But there are several methods how we can imitate this behaviour.
%To sample from a Hopfield network one takes a random input vector $\vec{v}$ and passes it through the network several times and updates each node after another and then determines the update of a node via the update rule $v_i = 1,~ if~\sum_j W_{ij} v_j \geq 0$. The outcome in this case is deterministic and one can stop this process after two iterations through the nodes did not lead to any change.

%\section{Learning: From Ising models to Boltzmann machines to restricted Boltzmann machines}
%
%\noindent\fbox{%
%    \parbox{\textwidth}{%
%       Somewhere I would like to add a comment on what can we do to train an energy based model. Because the acces to the "machine" is very limited and the only things we can do is adjust parameters and sample from the machine. Therefore the only thing we can do is sample and compare somehow to the actual data. And from this we have to derive an update rule.
%    }%
%}
%
%
%After one has agreed on a certain kind of model the learning or training procedure itself can be interpreted as the search for the parameters of the system Hamiltonian which maximize the entropy and fulfil the constraints. If we consider a Ising model the constraints are given by the expectation values $\langle \sigma_i \sigma_j \rangle $
% and $\langle \sigma_i \rangle$ and the coupling parameters $w_{i,j}$ of the Hamiltonian control the expectation values of $\langle \sigma_i \sigma_j \rangle $ and the local fields $h_i$ controll the single spin expectations $\langle \sigma_i \rangle$.
%To learn these system parameters we slowly change the parameters of the system until the expectation values of the model get closer of the expectation values of the data. 
%
%\textbf{In Figure such and such } one can try to adjust the parameters of the system in a way that the model distribution fits the target distribution (in a certain background color). One will see that this task is not easy because all the parameters influence each other and therefore one has to make small adjustments with each step.
%
%If we now go to a bit more difficult target distribution one will realize that it cannot be approximated with the model at hand. So far we only used a Ising system to model data, where every spin represents a data point. A much more expressive model is the Boltzmann machine, where we start to distinguish between visible and hidden spin variables. On first glance there is no difference between the two models except the name of the nodes. But the crucial point is that only the visible units are used to represent data. The hidden units are only there to increase the space of possible configurations and they will be averaged out in the end. This way we can increase the expressivity of the model. In Figure (such and such) one can try again to model the harder distribution but now with hidden nodes. You will see that it is now possible to approximate more complex target distributions.
%
%
%
%\subsection{R! BM and sampling}
%
% \textbf{Need to introduce RESTRICTED! BM, why do we do that? --> sampling}
%
%So far we assumed that we have access to the probability distribution of the energy based model $p(x, \theta)$ and the data $p(x)$. But in general this is not true because the partition function $Z$ is not tractable for systems of the size $N>20$. Therefore to compare the statistics of the data with the statistics of the model we need to be able to approximate these distributions. For the data this is relatively simple, because we can just average over some data instances, for example over batches. To approximate the model distribution we need configurations that come from the model itself. For a real physical system we therefore would set the parameters of the model and wait until it equilibrates, take many configurations like this and compare them to the data. 
%Unfortunately we cannot simulate these equilibration dynamics on a computer, because they are computationally very demanding. What we can do is Gibbs sampling. But for a BM Gibbs sampling is as well not tractable, because if we sample one variable a change would influence the probabilities of all other variables and equilibration would take very long time. \textbf{I am not 100 \% sure if it even would equilibrate} To avoid this problem the graph of the BM can be made bipartite, which means that we separate the nodes in a visible and a hidden layer and don't allow inter-layer connections. This architecture is called "Restricted Boltzmann Machine". With this simple trick the conditional probabilities $p(\vec{v}|\vec{h})$ and $p(\vec{h}|\vec{v})$ marginalize and therefore we can sample each node of a layer independent of the other nodes of the same layer. 
%
%The question that now remains is how can we efficiently adjust the parameters? Can we somehow derive an update rule simply by comparing the samples from the model with the data? The answer is yes and it is known under the name Contrastive Divergence. If we use the update rule $W \rightarrow W - \Delta W$ for gradient descent the update rule of CD is:
%
%\begin{equation*}
%	\Delta W_{i,j} = \langle \frac{ \partial E}{\partial W_{i,j}} \rangle_{\text{Data}} - \langle \frac{\partial E}{\partial W_{i,j}} \rangle_{p_{\theta}} %= \langle v_i h_j \rangle_{\text{Data}} - \langle v_i h_j \rangle_{p_{\theta}}
%\end{equation*}
%
%If the energy is given by $E(v,h) = \sum_{i,j} W_{i,j} v_i h_j $ (\textbf{sometimes there is a minus sign in front of the sum, which changes the direction}).
%
%But what is the intuition about this update rule, why is the minus sign in front of the average over the model distribution and not vice versa. How do we know in which direction we will have to move my parameters, if we train a model.
%To get some intuition about this update rule we consider the very extreme case of a system whit only two spins $v$ and $h$ and assume that these variables are always not equal in the data and always equal if we sample them from the distribution $p_{\theta}$. This means that $\langle v h \rangle_{\text{Data}} = -1$
% and $\langle v h \rangle_{p_{\theta}}  = 1$. So the update will be minimally, namely $\Delta W_{i,j} = -2$ and therefore we move to higher weights $W$. The data is always anti-aligned, which means the minimum energy of the configurations of the data will be achieved by increasing the weights to be strongly positive. And since the configurations sampled from the distribution $p_{\theta}$ are always aligned $W$ must be strongly negative at the moment because this has to be minimum energy.


\section{The physics of equilibrium (This is from Nathan's draft)}



Let's consider a simple thought exercise. Suppose we have a box which is engineered so that its contents are completely isolated from its exterior environment, except for a small aperture which we can open and close at will. We describe the state of the box with a small number of variables: its temperature $T$, its pressure $P$, and the number of particles of different types, $N_i$, that it has inside it. For simplicity, let's assume all these variables are set to zero initially. 

By opening the aperture, we can expose the interior of the box to the Earth's atmosphere. Some of the particles moving around in the ambient air will end up in the box, raising its internal temperature, its pressure, and its various particle concentrations. At first, there will be large fluctuations in these values, but over time they will stabilize. After giving enough time for the box to equilibrate, we close the aperture again. 

What is the new state of the box? From experience, we know the box will be at the same temperature and pressure, and contain the same concentrations of particles, as the ambient air. The atmosphere has, simply through interaction, imprinted its state onto the smaller box system. 
This illuminates a principle that we can ``imprint'' the state of one system onto another purely through a interaction. The box can been seen as a kind of proxy ``model'' of the environment, since it has the same values 

But why did the box's parameters -- the temperature, pressure, etc. -- change? Conservation of energy is one of the fundamental laws of nature, showing up across physics, chemistry, and biology. When two systems interact, they exchange energy. Any energy lost by one system will be offset by a gain in the other. In our example, 


which can cause the state of each system to change. This is a pretty broadly applicable rule, with variants appearing in physics, chemistry, biology, and machine learning.
Energy can be decomposed into a product of temperature $T$ and entropy $S$, i.e., $E = T\cdot S$ (these are an example of \emph{conjugate variables}). When a particle from the atmosphere enters the box, the box's entropy increases (



Technically, the box-atmosphere exchange goes both ways. Since some fraction of particles left the atmosphere and now reside in the box, the atmosphere is now technically at lower temperature, pressure, and particle concentrations than it was before interacting with the box. However, the \emph{scale} of the atmosphere is so gigantic compared to the box, that these changes would not even register within the precision of a detector (like a thermometer). The same story holds even if the box's initial parameters were nonzero. Whatever their value, interaction with the atmosphere will cause the box's state to shift to match the environment's, and vice-versa. 



\section{Plot ideas}
\begin{itemize}
 \item Histograms under the probability/energy curve; user can adjust gap (horizontal distance between histogram) and temperature (changes shape of probability/energy curve
 \item Schematic of a single 2-level system interacting with a ``reservoir'', states of the reservoir are sampled in a batch and interact with system, leading it to a specific average states
 \item Schematic of a RBM visible layer interacting with a stream of data examples, specifically images; can have similar feel to 2-level system interacting with reservoir
 \item Plot of a 2D plane showing different image configurations; their vertical location corresponds to how probable they are. Should have MNIST-like configurations at high prob, and random noise at low prob.
\end{itemize}


Equilibrium is one of the most universally applicable concepts in science. (Almost) everything equilibrates. Loosely put, equilibration is the process of systems interacting and finding a way to coexist. When two physical systems interact, they can exchange energy (a kind of universal currency). Conservation of energy is such a strong principle in physics that is has been elevated to a kind of natural \emph{law}.

\begin{thebibliography}{10}
\bibitem{carleo2017solving}
G.~Carleo, M.~Troyer, {\it Science\/} {\bf 355, 6325}, 602
  (2017).
\end{thebibliography}



\newpage
\section{Notes from March}
\subsection{Outline}
\begin{itemize}
\item The starting point is a derivation, using the maximum entropy principle, of the probability distribution that characterizes energy-based models.
\item Novelty here is that we do not introduce an energy function; rather, we introduce a set of constraints based on statistics of our data. 
\item In the specific case of a distribution over $n$-bit strings $z$, setting constraints on the expectation values $\langle \sigma_i \rangle$, $\langle \sigma_i \sigma_j\rangle$ is enough to derive the Boltzmann distribution for the Ising model. This immediately implies that these expectations are sufficient to determine the model parameters.
\item We can then make a connection to maximum likelihood estimation (MLE), where given a model specified by a Boltzmann distribution, the maximum likelihood parameters of the model are those that match the expectation values of the data, thus motivating our choice of constraints.
\item The above constraints could (somehow) be interpreted as `thermalizing with a bath of data', but we concluded this was too confusing.
\item Instead, we make a connection to physics, showing that the previous derivation of our model is equivalent to the distribution at thermal equilibrium of a system with a given energy function, i.e., a given Hamiltonian.
\item Here it is possible to use some of the visualizations to illustrate the concept of thermal distribution and the connection between low energy and high probability.
\item At this stage, we can interpret our task of training a model with finding the correct Hamiltonian parameters. Crucially, each parameter controls a corresponding expectation value, e.g. $w_{ij}$ controls $\langle \sigma_i \sigma_j$.
\item Therefore, we can train our model by slowly changing each parameter such that its corresponding expectation gets closer to the expectation for the data. This approach is exactly what we do in contrastive divergence, which we have therefore derived without the need to take derivatives of the relative entropy.
\item An interactive visualization of this training process would be extremely helpful here: give the reader the task of finding parameters that match all expectation values. We can hint that a good strategy is to slowly change each one independently.
\item We can then argue that there is not enough richness in the probability distribution. A better strategy is to think of a large system, and use as a model the probability distribution of a subsystem. This motivates the partitioning into hidden and visible layers. This is another opportunity for a visualization to motivate this choice.
\item So far, we have assumed that we have access to the expectation values for our model. In the absence of an actual physical device, our only choice is to simulate sampling from its Boltzmann distribution. We do this with Gibbs sampling.
\end{itemize}

From here onwards the path is less clear, but the following are some ideas
\begin{itemize}
\item Gibbs sampling is difficult in general, but becomes much easier when we restrict the interactions to occur only between visible and hidden layers. Hence the birth of RBMs.
\item This is a good opportunity to introduce a visualization of how Gibbs sampling works for RBMs. Can it also be interpreted as equilibration between layers?
\item Application: image reconstruction. Discuss how this can also be viewed as a high-energy out-of-equilibrium state undergoing thermalization. A good visualization is also a possibility here.
\item Smuggle in quantum? We go crazy and think of matrix-based energy models. One idea would be to show how they are strictly richer models.
\end{itemize}

Below are more detailed notes on the concepts we are planning to discuss.




\section{Maximum Entropy Principle}

\subsection{Theory}

If we maximize the entropy of the probability distribution $p(x)$ that describes the random variables $\{x_i\}_i^N$ under the constarints $\sum_i p(x_i) f_k(x_i) = \langle f_k \rangle$ we automatically obtain the Boltzmann distribution 

\begin{equation*}
p(x) = 1/Z \sum_i^N \exp(\sum_k \lambda_k f_k(x_i)).
\end{equation*} 

Where $f_k(x_i)$ are $m$ arbitrary functions of the random variables $x_i$ and $\lambda_k$ are the Lagrange multipliers . An additional constraint is that $\sum_i p(x_i) = 1$.

\subsection{Applied to physics}

In a physical system the constraints are given by the Hamiltonian of the system. For example through the expectation values of single site observables and two-body correlators.

\begin{align*}
	\langle \sigma_i \rangle &= \sum_i p(\sigma_i) \sigma_i \\
	\langle \sigma_i \sigma_j \rangle &= \sum_{i,j} p(\sigma_i, \sigma_j) \sigma_i \sigma_j 
\end{align*}

These constraints come directly from the Hamiltonian $H = \sum_i b_i \sigma_i  + \sum_{i,j} J_{i,j} \sigma_i \sigma_j$.


\textcolor{red}{Here are several things still unclear. Why does this meanfield approach apply? Are there other constraints we could implement?} One way to explain it is probably the sufficiency of the statistic. Which would mean that these two expectation values are sufficient statistics for the Boltzmann distribution. But this would mean, that the Boltzmann distribution comes also from the fact that we chose these statistics / constraints.

\subsubsection{Sufficient statistic}
A statistic $t(\vec{x})$ is sufficient with respect to a statistical model $p_{\theta}(\vec{x})$ and its parameter $\theta$ if  the conditional probability $p(\vec{x}|(t(\vec{x}))$ does not depend on $\theta$. Where $\vec{x}$ is a sample $(x_1, \dots x_n)$ of random variables. In other words, a statistic is sufficient with respect to a statistical model and the parameter $\theta$, if there is no way of extracting any additional information from the sample (concerning parameter $\theta$) than the staistic $t(\vec{x})$ already conveis.

A simple example: If we choose to describe a set of random variables by a Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$, the mean of $\vec{x}$ is a sufficient statistic for the parameter $\mu$ .

\subsection{Connection to machine learning}
In ML we normally choose the model we would like to fit, which is in this case a Boltzmann distribution $p = \exp(\sum_k w_k f_k)$ and then we fit the data $(x_1, \dots x_n)$ such that $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i \sigma_j \rangle_{\text{Data}}$ and $\langle \sigma_i \rangle_p = \langle \sigma_i  \rangle_{\text{Data}}$. To do so we apply several tricks like making BM bipartite and use sampling to get approximate the model distribution.

\subsection{Going from this theory to a trainable model}

\begin{itemize}
	\item We will start with a visualization of  $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i \sigma_j \rangle_{\text{Data}}$ for a general energy based model where we don't have hidden and visible units. The input data just has the same dimension as the number of spins. The expectation values for the data are therefore fixed and we change the model parameters with sliders and the reader should try to reproduce the data distribution by changing the model parameters with these sliders.
	\item In a next step we can make a visualization where we do something similar but with a task that cannot be achieved. For example we have a 4 dimensional data set (4 pixels) and we want some configurations to be very likely and some very similar configurations have to be almost 0 probability. We will show that this cannot be achieved just by a system of 4 spins. Therefore we introduce hidden units which are just traced out in the end.
	\item With such a model (which is a Boltzmann machine) we actual have more expressive power but this power comes with the price that we have to sample the hidden units now also for the $\langle \sigma_i \sigma_j \rangle_{\text{Data}}$ part, which we now write as $\langle v_i h_j \rangle_{\text{Data}}$.
	\item We can now also add the same figure from step 1) again, but now the task is a bit trickier, because the target distribution $\langle \sigma_i \sigma_j \rangle_{\text{Data}} \rightarrow \langle v_i h_j \rangle_{\text{Data}}$ is not fixed anymore, because it also contains random variables $h_j$. Therefore it will be a bit harder to fit $\langle v_i h_j \rangle_p = \langle v_i h_j \rangle_{\text{Data}}$.
	\item from here on we can also explain for example why we do Restricted BMs. Why they are bipartite? Because of this the conditional probabilities $p(\vec{v} | \vec{h})$ and $p(\vec{h} | \vec{v})$ marginalize. And finally why contrastive divergence?
	\item \textcolor{red}{We might mention Hopfield networks in this section?}
\end{itemize}

\subsection{Equilibration with bath of data}

The formula $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i  \sigma_j \rangle_{\text{Data}}$ made us think of this process as a thermalization with a bath of data, where the system has to equlibrate with this bath. But for each pair of spins we would need a different bath and the equilibrate temperature would be the parameter $W_{i,j}$. For the moment we decided that this picture does not help to understand BMs better. It is rather confusing and complicated.

\subsection{Contrastive Divergence}

The update rule of an RBM is:

\begin{equation*}
	\Delta W_{i,j} = \langle \frac{ \partial E}{\partial W_{i,j}} \rangle_{\text{Data}} - \langle \frac{\partial E}{\partial W_{i,j}} \rangle_{p_{\theta}} = \langle v_i h_j \rangle_{\text{Data}} - \langle v_i h_j \rangle_{p_{\theta}}
\end{equation*}

If the energy is given by $E(v,h) = \sum_{i,j} W_{i,j} v_i h_j $ (\textcolor{red}{sometimes there is a minus sign in front of the sum, which changes the direction}), then the update rule is $W \rightarrow W - \Delta W$

The question here was, why is the minus sign in front of the average over the model distribution and not vice versa. How do I know in which direction I will have to move my parameters, if I train a model.
There is a intuitive approach to this question if we consider the very extreme case of a system whith only two spins $v$ and $h$ and we assume that these variables are always not equal in the data and always equal if we sample them from the distribution $p_{\theta}$. This means that $\langle v_i h_j \rangle_{\text{Data}} = -1$
 and $\langle v_i h_j \rangle_{p_{\theta}}  = 1$. So the update will be minimally, namely $\Delta W_{i,j} = -2$. And this makes physically sense because the data is always anti-alligned, which means the minimum energy of the configurations of the data will be achieved by increasing the weights. Since the configurations of the distribution $p_{\theta}$ are always aligned to achieve minimum energy $W$ must be strongly negative.
 
 
\end{document}
