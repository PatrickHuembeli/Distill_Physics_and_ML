\documentclass[nofootinbib, superscriptaddress, prl]{revtex4}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{braket}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}
\title{Physics of Equilibrium}
\author{Patrick, Juan-Miguel, Arthur, Peter, Nathan}
\maketitle

\section{Outline}
\begin{itemize}
\item The starting point is a derivation, using the maximum entropy principle, of the probability distribution that characterizes energy-based models.
\item Novelty here is that we do not introduce an energy function; rather, we introduce a set of constraints based on statistics of our data. 
\item In the specific case of a distribution over $n$-bit strings $z$, setting constraints on the expectation values $\langle \sigma_i \rangle$, $\langle \sigma_i \sigma_j\rangle$ is enough to derive the Boltzmann distribution for the Ising model. This immediately implies that these expectations are sufficient to determine the model parameters.
\item We can then make a connection to maximum likelihood estimation (MLE), where given a model specified by a Boltzmann distribution, the maximum likelihood parameters of the model are those that match the expectation values of the data, thus motivating our choice of constraints.
\item The above constraints could (somehow) be interpreted as `thermalizing with a bath of data', but we concluded this was too confusing.
\item Instead, we make a connection to physics, showing that the previous derivation of our model is equivalent to the distribution at thermal equilibrium of a system with a given energy function, i.e., a given Hamiltonian.
\item Here it is possible to use some of the visualizations to illustrate the concept of thermal distribution and the connection between low energy and high probability.
\item At this stage, we can interpret our task of training a model with finding the correct Hamiltonian parameters. Crucially, each parameter controls a corresponding expectation value, e.g. $w_{ij}$ controls $\langle \sigma_i \sigma_j$.
\item Therefore, we can train our model by slowly changing each parameter such that its corresponding expectation gets closer to the expectation for the data. This approach is exactly what we do in contrastive divergence, which we have therefore derived without the need to take derivatives of the relative entropy.
\item An interactive visualization of this training process would be extremely helpful here: give the reader the task of finding parameters that match all expectation values. We can hint that a good strategy is to slowly change each one independently.
\item We can then argue that there is not enough richness in the probability distribution. A better strategy is to think of a large system, and use as a model the probability distribution of a subsystem. This motivates the partitioning into hidden and visible layers. This is another opportunity for a visualization to motivate this choice.
\item So far, we have assumed that we have access to the expectation values for our model. In the absence of an actual physical device, our only choice is to simulate sampling from its Boltzmann distribution. We do this with Gibbs sampling.
\end{itemize}

From here onwards the path is less clear, but the following are some ideas
\begin{itemize}
\item Gibbs sampling is difficult in general, but becomes much easier when we restrict the interactions to occur only between visible and hidden layers. Hence the birth of RBMs.
\item This is a good opportunity to introduce a visualization of how Gibbs sampling works for RBMs. Can it also be interpreted as equilibration between layers?
\item Application: image reconstruction. Discuss how this can also be viewed as a high-energy out-of-equilibrium state undergoing thermalization. A good visualization is also a possibility here.
\item Smuggle in quantum? We go crazy and think of matrix-based energy models. One idea would be to show how they are strictly richer models.
\end{itemize}

Below are more detailed notes on the concepts we are planning to discuss.




\section{Maximum Entropy Principle}

\subsection{Theory}

If we maximize the entropy of the probability distribution $p(x)$ that describes the random variables $\{x_i\}_i^N$ under the constarints $\sum_i p(x_i) f_k(x_i) = \langle f_k \rangle$ we automatically obtain the Boltzmann distribution 

\begin{equation*}
p(x) = 1/Z \sum_i^N \exp(\sum_k \lambda_k f_k(x_i)).
\end{equation*} 

Where $f_k(x_i)$ are $m$ arbitrary functions of the random variables $x_i$ and $\lambda_k$ are the Lagrange multipliers . An additional constraint is that $\sum_i p(x_i) = 1$.

\subsection{Applied to physics}

In a physical system the constraints are given by the Hamiltonian of the system. For example through the expectation values of single site observables and two-body correlators.

\begin{align*}
	\langle \sigma_i \rangle &= \sum_i p(\sigma_i) \sigma_i \\
	\langle \sigma_i \sigma_j \rangle &= \sum_{i,j} p(\sigma_i, \sigma_j) \sigma_i \sigma_j 
\end{align*}

These constraints come directly from the Hamiltonian $H = \sum_i b_i \sigma_i  + \sum_{i,j} J_{i,j} \sigma_i \sigma_j$.


\textcolor{red}{Here are several things still unclear. Why does this meanfield approach apply? Are there other constraints we could implement?} One way to explain it is probably the sufficiency of the statistic. Which would mean that these two expectation values are sufficient statistics for the Boltzmann distribution. But this would mean, that the Boltzmann distribution comes also from the fact that we chose these statistics / constraints.

\subsubsection{Sufficient statistic}
A statistic $t(\vec{x})$ is sufficient with respect to a statistical model $p_{\theta}(\vec{x})$ and its parameter $\theta$ if  the conditional probability $p(\vec{x}|(t(\vec{x}))$ does not depend on $\theta$. Where $\vec{x}$ is a sample $(x_1, \dots x_n)$ of random variables. In other words, a statistic is sufficient with respect to a statistical model and the parameter $\theta$, if there is no way of extracting any additional information from the sample (concerning parameter $\theta$) than the staistic $t(\vec{x})$ already conveis.

A simple example: If we choose to describe a set of random variables by a Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$, the mean of $\vec{x}$ is a sufficient statistic for the parameter $\mu$ .

\subsection{Connection to machine learning}
In ML we normally choose the model we would like to fit, which is in this case a Boltzmann distribution $p = \exp(\sum_k w_k f_k)$ and then we fit the data $(x_1, \dots x_n)$ such that $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i \sigma_j \rangle_{\text{Data}}$ and $\langle \sigma_i \rangle_p = \langle \sigma_i  \rangle_{\text{Data}}$. To do so we apply several tricks like making BM bipartite and use sampling to get approximate the model distribution.

\subsection{Going from this theory to a trainable model}

\begin{itemize}
	\item We will start with a visualization of  $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i \sigma_j \rangle_{\text{Data}}$ for a general energy based model where we don't have hidden and visible units. The input data just has the same dimension as the number of spins. The expectation values for the data are therefore fixed and we change the model parameters with sliders and the reader should try to reproduce the data distribution by changing the model parameters with these sliders.
	\item In a next step we can make a visualization where we do something similar but with a task that cannot be achieved. For example we have a 4 dimensional data set (4 pixels) and we want some configurations to be very likely and some very similar configurations have to be almost 0 probability. We will show that this cannot be achieved just by a system of 4 spins. Therefore we introduce hidden units which are just traced out in the end.
	\item With such a model (which is a Boltzmann machine) we actual have more expressive power but this power comes with the price that we have to sample the hidden units now also for the $\langle \sigma_i \sigma_j \rangle_{\text{Data}}$ part, which we now write as $\langle v_i h_j \rangle_{\text{Data}}$.
	\item We can now also add the same figure from step 1) again, but now the task is a bit trickier, because the target distribution $\langle \sigma_i \sigma_j \rangle_{\text{Data}} \rightarrow \langle v_i h_j \rangle_{\text{Data}}$ is not fixed anymore, because it also contains random variables $h_j$. Therefore it will be a bit harder to fit $\langle v_i h_j \rangle_p = \langle v_i h_j \rangle_{\text{Data}}$.
	\item from here on we can also explain for example why we do Restricted BMs. Why they are bipartite? Because of this the conditional probabilities $p(\vec{v} | \vec{h})$ and $p(\vec{h} | \vec{v})$ marginalize. And finally why contrastive divergence?
	\item \textcolor{red}{We might mention Hopfield networks in this section?}
\end{itemize}

\subsection{Equilibration with bath of data}

The formula $\langle \sigma_i \sigma_j \rangle_p = \langle \sigma_i  \sigma_j \rangle_{\text{Data}}$ made us think of this process as a thermalization with a bath of data, where the system has to equlibrate with this bath. But for each pair of spins we would need a different bath and the equilibrate temperature would be the parameter $W_{i,j}$. For the moment we decided that this picture does not help to understand BMs better. It is rather confusing and complicated.

\subsection{Contrastive Divergence}

The update rule of an RBM is:

\begin{equation*}
	\Delta W_{i,j} = \langle \frac{ \partial E}{\partial W_{i,j}} \rangle_{\text{Data}} - \langle \frac{\partial E}{\partial W_{i,j}} \rangle_{p_{\theta}} = \langle v_i h_j \rangle_{\text{Data}} - \langle v_i h_j \rangle_{p_{\theta}}
\end{equation*}

If the energy is given by $E(v,h) = \sum_{i,j} W_{i,j} v_i h_j $ (\textcolor{red}{sometimes there is a minus sign in front of the sum, which changes the direction}), then the update rule is $W \rightarrow W - \Delta W$

The question here was, why is the minus sign in front of the average over the model distribution and not vice versa. How do I know in which direction I will have to move my parameters, if I train a model.
There is a intuitive approach to this question if we consider the very extreme case of a system whith only two spins $v$ and $h$ and we assume that these variables are always not equal in the data and always equal if we sample them from the distribution $p_{\theta}$. This means that $\langle v_i h_j \rangle_{\text{Data}} = -1$
 and $\langle v_i h_j \rangle_{p_{\theta}}  = 1$. So the update will be minimally, namely $\Delta W_{i,j} = -2$. And this makes physically sense because the data is always anti-alligned, which means the minimum energy of the configurations of the data will be achieved by increasing the weights. Since the configurations of the distribution $p_{\theta}$ are always aligned to achieve minimum energy $W$ must be strongly negative.
 
 
\end{document}